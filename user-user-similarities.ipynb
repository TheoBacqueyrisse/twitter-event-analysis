{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings and User - User Similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages ðŸ“¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: node2vec in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.4.6)\n",
      "Requirement already satisfied: gensim<5.0.0,>=4.1.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from node2vec) (4.3.2)\n",
      "Requirement already satisfied: joblib<2.0.0,>=1.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from node2vec) (1.3.2)\n",
      "Requirement already satisfied: networkx<3.0,>=2.5 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from node2vec) (2.8.8)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.19.5 in /home/codespace/.local/lib/python3.10/site-packages (from node2vec) (1.26.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.55.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from node2vec) (4.66.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/codespace/.local/lib/python3.10/site-packages (from gensim<5.0.0,>=4.1.2->node2vec) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gensim<5.0.0,>=4.1.2->node2vec) (7.0.3)\n",
      "Requirement already satisfied: wrapt in /usr/local/python/3.10.13/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.1.2->node2vec) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gensim in /usr/local/python/3.10.13/lib/python3.10/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/codespace/.local/lib/python3.10/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/codespace/.local/lib/python3.10/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gensim) (7.0.3)\n",
      "Requirement already satisfied: wrapt in /usr/local/python/3.10.13/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "\n",
    "%pip install node2vec\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "%pip install gensim\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the event to be analyzed, and import the graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_event = 'wildfire'\n",
    "\n",
    "path = f'subgraphs_data/{chosen_event}_subgraph.graphml'\n",
    "g = nx.read_graphml(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Sampling N users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we need access to the list of users in the graph \n",
    "users = []\n",
    "for node, data in g.nodes(data=True):\n",
    "    for key, value in data.items():\n",
    "        if key == 'labels':\n",
    "            if value == ':User':\n",
    "                users.append({node : data})\n",
    "\n",
    "# then, we select a random sample of 100 users\n",
    "random.seed(55)\n",
    "N = 100\n",
    "users_sample = random.sample(users, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n87752': {'labels': ':User',\n",
       "  'listed_count': 1,\n",
       "  'statuses_count': 4943,\n",
       "  'favourites_count': 23676,\n",
       "  'isVerified': False,\n",
       "  'screen_name': 'gillian__paige',\n",
       "  'friends_count': 211,\n",
       "  'followers_count': 132,\n",
       "  'name': 'gillian hebert',\n",
       "  'tweets_count': 1,\n",
       "  'id': '1873350745'}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of user\n",
    "users_sample[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.ii.a - Embeddings on Graph Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we need the associated nodes ids to the sample we definded : \n",
    "node_sample_ids = []\n",
    "for i in range(len(users_sample)):\n",
    "    for key, _ in users_sample[i].items():\n",
    "        node_sample_ids.append(key)\n",
    "\n",
    "# define the subgraph with the sample of 100 nodes\n",
    "sample_graph = g.subgraph(node_sample_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 35496.82it/s]\n",
      "Generating walks (CPU: 1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 1456.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# we can then build the embeddings of the sampled graph with Node2Vec\n",
    "node2vec = Node2Vec(sample_graph, dimensions=50)\n",
    "fitted_model_n2v = node2vec.fit(window=10, min_count=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have access to the 10 most similar nodes of a node, based on cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('n85976', 0.44765573740005493),\n",
       " ('n99683', 0.373142808675766),\n",
       " ('n85300', 0.3534637689590454),\n",
       " ('n98362', 0.27789026498794556),\n",
       " ('n87000', 0.2662959694862366),\n",
       " ('n85755', 0.26048803329467773),\n",
       " ('n99430', 0.2583834230899811),\n",
       " ('n87335', 0.2576662600040436),\n",
       " ('n87158', 0.24259963631629944),\n",
       " ('n87394', 0.23346398770809174)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_model_n2v.wv.most_similar('n98232')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can build a cosine similarity matrix :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the embeddings for every node\n",
    "node_embeddings_n2v = {node: fitted_model_n2v.wv[node] for node in sample_graph.nodes()}\n",
    "\n",
    "# list element is easier to handle\n",
    "list_of_embeddings_n2v = []\n",
    "for key, value in node_embeddings_n2v.items():\n",
    "    list_of_embeddings_n2v.append({key : value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the cosine similarity matrix\n",
    "cos_sim_n2v = [[0 for _ in range(100)] for _ in range(100)]\n",
    "\n",
    "for i, emb_i_dict in enumerate(list_of_embeddings_n2v):\n",
    "    for j, emb_j_dict in enumerate(list_of_embeddings_n2v):\n",
    "        emb_i = next(iter(emb_i_dict.values())) \n",
    "        emb_j = next(iter(emb_j_dict.values()))\n",
    "\n",
    "        cosine_sim = cosine_similarity([emb_i], [emb_j])[0][0]\n",
    "        cos_sim_n2v[i][j] = cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 more similar pairs of users are :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(3, 92, 0.5292955),\n",
       " (6, 62, 0.48984912),\n",
       " (92, 96, 0.4476558),\n",
       " (5, 44, 0.43664718),\n",
       " (64, 92, 0.4299842),\n",
       " (22, 44, 0.42970902),\n",
       " (7, 70, 0.40657914),\n",
       " (88, 92, 0.39449665),\n",
       " (71, 95, 0.3878101),\n",
       " (13, 18, 0.38690877)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from this, we can get the more similar users in our sample graph :\n",
    "arr_cos_sim_n2v = np.array(cos_sim_n2v)\n",
    "np.fill_diagonal(arr_cos_sim_n2v, -np.inf) # we replace the 1 elements of the diagonal by -inf\n",
    "\n",
    "# 10 most similar nodes\n",
    "v = []\n",
    "\n",
    "nb_users_to_print = 10\n",
    "for _ in range(nb_users_to_print):\n",
    "    max_index = np.argmax(arr_cos_sim_n2v)\n",
    "    max_sim = np.max(arr_cos_sim_n2v)\n",
    "\n",
    "    max_row_index, max_col_index = np.unravel_index(max_index, np.array(arr_cos_sim_n2v).shape)\n",
    "\n",
    "    arr_cos_sim_n2v[max_row_index][max_col_index] = -np.inf\n",
    "    arr_cos_sim_n2v[max_col_index][max_row_index] = -np.inf\n",
    "\n",
    "    v.append((max_row_index, max_col_index, max_sim))\n",
    "\n",
    "print(f\"The {nb_users_to_print} more similar pairs of users are :\")\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.ii.b - Embeddings on Post Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n37162': {'labels': ':Tweet',\n",
       "  'is_quote_status': False,\n",
       "  'possibly_sensitive': True,\n",
       "  'retweet_count': 37,\n",
       "  'favorite_count': 214,\n",
       "  'id_str': '1131945414289006592',\n",
       "  'isTruncated': False,\n",
       "  'annotation_postPriority': 'High',\n",
       "  'created_at': '2019-05-24T00:00Z',\n",
       "  'id': '1131945414289006592',\n",
       "  'annotation_annotated': True,\n",
       "  'annotation_num_judgements': 3,\n",
       "  'text': \"I support High Level Mayor Crystal McAteer and her call for emergency relief funding in the form of debit cards for residents displaced by the fire. I'll be reaching out to Minister Madu today and call on him to bring this program in immediately#ableg\",\n",
       "  'topic': 'TRECIS-CTIT-H-029'}}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we already have our sampled graph and the users, we now need the tweets\n",
    "tweets = []\n",
    "for node, data in g.nodes(data=True):\n",
    "    for key, value in data.items():\n",
    "        if key == 'labels':\n",
    "            if value == ':Tweet':\n",
    "                tweets.append({node : data})\n",
    "\n",
    "# exemple of tweet\n",
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user': 'n87752',\n",
       "  'tweets': ['literally Jason Kenney and the UCP right now #ABwildfire #AbLeg https://t.co/g3mNEpci2H']},\n",
       " {'user': 'n98232',\n",
       "  'tweets': ['View of Springwood fire looking toward Winmalee from Warrimoo Oval #nswfires #nswrfs http://t.co/ffIvaobke2']}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a list containing dictionnaries with the user_node_id and every tweet made by this user\n",
    "users_posts = []\n",
    "\n",
    "for u in users_sample:\n",
    "    user_node_id = [key for key, _ in u.items()][0]\n",
    "    tweets_by_user = []\n",
    "    \n",
    "    for t in tweets:\n",
    "        tweet_node_id = [key for key, _ in t.items()][0]\n",
    "        text_tweet = [value for _, value in t.items()][0]['text']\n",
    "\n",
    "        if tweet_node_id in g[user_node_id]:\n",
    "            tweets_by_user.append(text_tweet)\n",
    "\n",
    "    users_posts.append({'user':user_node_id,\n",
    "                        'tweets':tweets_by_user})\n",
    "    \n",
    "# exemple of users posts :\n",
    "users_posts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 199.5/199.5MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# let us build the empbeddings of our tweets, using a pretrained Word2Vec model on twitter data, using embeddings of length 50 to match the graph structure embeddings\n",
    "w2v = gensim.downloader.load('glove-twitter-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define a function that allows us to tokenize a tweet/sentence, and we take the average of each embedding\n",
    "def sentence_embedding(sentence):\n",
    "    tokens = sentence.split()\n",
    "    embeddings = [w2v[token] for token in tokens if token in w2v]\n",
    "    \n",
    "    if embeddings:\n",
    "        avg_embedding = np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        avg_embedding = np.zeros(w2v.vector_size)\n",
    "\n",
    "    return avg_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we then compute and add the embedded tweets to the users_posts list, by adding a key to each dictionnary in the list\n",
    "for i in users_posts:\n",
    "    tweets = i['tweets']\n",
    "    embedded_tweets = []\n",
    "    for j in i['tweets']:\n",
    "        emb_tweet = sentence_embedding(j)\n",
    "        embedded_tweets.append(emb_tweet)\n",
    "    avg_emb_tweet = np.mean(embedded_tweets, axis=0)\n",
    "    i['embedded_tweets'] = avg_emb_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user': 'n87752',\n",
       " 'tweets': ['literally Jason Kenney and the UCP right now #ABwildfire #AbLeg https://t.co/g3mNEpci2H'],\n",
       " 'embedded_tweets': array([ 0.092144  , -0.0097482 ,  0.2523414 , -0.412006  , -0.110684  ,\n",
       "         0.381162  ,  1.1097358 ,  0.194222  , -0.10341058, -0.159414  ,\n",
       "         0.2009306 , -0.3376276 , -5.7268    , -0.26993603,  0.011716  ,\n",
       "        -0.16570339,  0.17271021, -0.2074794 , -0.384662  , -0.1488848 ,\n",
       "        -0.12701042, -0.09818981,  0.057602  ,  0.19408801,  0.08912279,\n",
       "         0.520362  , -0.15948038,  0.01510841,  0.363132  , -0.02014689,\n",
       "        -0.396355  ,  0.170776  , -0.2349926 , -0.04268146,  0.20499201,\n",
       "         0.07206976,  0.064438  , -0.0277014 ,  0.27153403,  0.20733199,\n",
       "        -0.672186  ,  0.031954  , -0.018946  ,  0.108678  ,  0.19266601,\n",
       "        -0.117673  ,  0.20545301, -0.060766  , -0.1335644 ,  0.04661779],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exemple of new user post :\n",
    "users_posts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now compute the cosine similarity matrix like with our graph structure embeddings\n",
    "cos_sim_w2v = [[0 for _ in range(100)] for _ in range(100)]\n",
    "\n",
    "for i in range(len(users_posts)):\n",
    "    for j in range(len(users_posts)):\n",
    "        if i == j:\n",
    "            cos_sim_w2v[i][j] = 1\n",
    "        else:\n",
    "            emb_i = users_posts[i]['embedded_tweets'].reshape(1, -1)\n",
    "            emb_j = users_posts[j]['embedded_tweets'].reshape(1, -1)\n",
    "            cos_sim_w2v[i][j] = cosine_similarity(emb_i, emb_j)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 more similar pairs of users are :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(57, 71, 0.9946486949920654),\n",
       " (71, 92, 0.9932003021240234),\n",
       " (92, 99, 0.9930979609489441),\n",
       " (20, 75, 0.9925435185432434),\n",
       " (71, 99, 0.9906183481216431),\n",
       " (20, 38, 0.9901840090751648),\n",
       " (33, 57, 0.9893929958343506),\n",
       " (22, 77, 0.9890386462211609),\n",
       " (72, 99, 0.9882469773292542),\n",
       " (31, 92, 0.988242506980896)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finally, we can print the most similar users based on this matrix\n",
    "arr_cos_sim_w2v = np.array(cos_sim_w2v)\n",
    "np.fill_diagonal(arr_cos_sim_w2v, -np.inf) # replace the diagonal full of 1s by -inf \n",
    "\n",
    "v = []\n",
    "\n",
    "nb_users_to_print = 10\n",
    "for _ in range(nb_users_to_print):\n",
    "    max_index = np.argmax(arr_cos_sim_w2v)\n",
    "    max_sim = np.max(arr_cos_sim_w2v)\n",
    "\n",
    "    max_row_index, max_col_index = np.unravel_index(max_index, np.array(arr_cos_sim_w2v).shape)\n",
    "\n",
    "    arr_cos_sim_w2v[max_row_index][max_col_index] = -np.inf\n",
    "    arr_cos_sim_w2v[max_col_index][max_row_index] = -np.inf\n",
    "    \n",
    "    v.append((max_row_index, max_col_index, max_sim))\n",
    "\n",
    "print(f\"The {nb_users_to_print} more similar pairs of users are :\")\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Worst #AirQuality I?ve ever experienced.2.5 weeks on the road in the US talking and learning about #HealthAndWellbeing and return to campfire air from #ABFire. Stay inside, run your #Purifiers and/or go to a place with filtered air. Stop #running outside, please! #ClimateChange', \"#AQHI in #YEG #Edmonton is 10+. That. is. bad! Tomorrow it's going down to 5 and the forecast looks promising for this area but continues to be horrible for the North. #ABFire #ThisCantBeTheNewNormal #ClimateChange\"]\n",
      "\n",
      "['@EdLatimore Yeah man! It?s wild\\\\n\\\\nI worked a couple days in -40 in northern Alberta and that?s what I found\\\\n\\\\nWhen the air is still, it feels like nothing\\\\n\\\\nAs soon as you start to walk, your face is on fire']\n"
     ]
    }
   ],
   "source": [
    "# let us look at users 71 and 92 posts (post 57 is very long so bit hard to analyze)\n",
    "print(users_posts[71]['tweets'])\n",
    "print()\n",
    "print(users_posts[92]['tweets'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the wildifre subgraph for example, it seems that both tweets are talking mainly about air quality specifically !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.iii - Trends in Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
