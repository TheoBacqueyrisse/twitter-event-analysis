{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings and User - User Similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages ðŸ“¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: node2vec in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.4.6)\n",
      "Requirement already satisfied: gensim<5.0.0,>=4.1.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from node2vec) (4.3.2)\n",
      "Requirement already satisfied: joblib<2.0.0,>=1.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from node2vec) (1.3.2)\n",
      "Requirement already satisfied: networkx<3.0,>=2.5 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from node2vec) (2.8.8)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.19.5 in /home/codespace/.local/lib/python3.10/site-packages (from node2vec) (1.26.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.55.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from node2vec) (4.66.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/codespace/.local/lib/python3.10/site-packages (from gensim<5.0.0,>=4.1.2->node2vec) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gensim<5.0.0,>=4.1.2->node2vec) (7.0.3)\n",
      "Requirement already satisfied: wrapt in /usr/local/python/3.10.13/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.1.2->node2vec) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gensim in /usr/local/python/3.10.13/lib/python3.10/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/codespace/.local/lib/python3.10/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/codespace/.local/lib/python3.10/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gensim) (7.0.3)\n",
      "Requirement already satisfied: wrapt in /usr/local/python/3.10.13/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%pip install node2vec\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "%pip install gensim\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the event to be analyzed, and import the graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_event = 'earthquake'\n",
    "\n",
    "# path = f'C:/Users/Utilisateur/Documents/M2/Web Mining/Projet/subgraph/{chosen_event}_subgraph.graphml'\n",
    "path = f'subgraphs_data/{chosen_event}_subgraph.graphml'\n",
    "g = nx.read_graphml(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Sampling N users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n96392': {'labels': ':User',\n",
       "  'isVerified': True,\n",
       "  'followers_count': 41496,\n",
       "  'listed_count': 1098,\n",
       "  'statuses_count': 6046,\n",
       "  'favourites_count': 292,\n",
       "  'id': '105153029',\n",
       "  'screen_name': 'CNNImpact',\n",
       "  'friends_count': 420,\n",
       "  'name': 'Impact Your World',\n",
       "  'tweets_count': 1}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = []\n",
    "nodes_users = []\n",
    "for node, data in g.nodes(data=True):\n",
    "    for key, value in data.items():\n",
    "        if key == 'labels':\n",
    "            if value == ':User':\n",
    "                nodes_users.append(node)\n",
    "                users.append({node : data})\n",
    "\n",
    "#We choose the size of the sample\n",
    "n=100\n",
    "\n",
    "# Sample n/2 users with highest degree centrality\n",
    "degree_centralities = nx.degree_centrality(g) #We first compute the degree coef fo all nodes\n",
    "degree_filtred = {key: value for key, value in degree_centralities.items() if key in nodes_users} #We then only select the users nodes\n",
    "top_degree_users = sorted(degree_filtred, key=lambda x: degree_filtred[x], reverse=True)[0:int(n/2)] #We take the 50 users that have the highest degree centrality\n",
    "\n",
    "\n",
    "filtered_nodes = [node for node in nodes_users if node not in top_degree_users] #We create a list of all the users except the ones already in the top 50 of degree centrality\n",
    "\n",
    "# Sample n/2 users with highest closeness centrality (taking away the nodes that are already in the highest degree)\n",
    "closeness_centralities = nx.closeness_centrality(g) #We first compute the closeness coef fo all nodes\n",
    "closeness_filtred = {key: value for key, value in closeness_centralities.items() if key in filtered_nodes} #We then only select the users nodes (that are not in the top 50 degree centrality)\n",
    "top_closeness_users = sorted(closeness_filtred, key=lambda x: closeness_filtred[x], reverse=True)[0:int(n/2)]  #We take the 50 users that have the highest closeness centrality\n",
    "\n",
    "# List of the selected users\n",
    "list_user = list(set(top_degree_users + top_closeness_users))\n",
    "\n",
    "users_sample = []\n",
    "\n",
    "for node_dict in users:\n",
    "    node_key = list(node_dict.keys())[0]  \n",
    "    if node_key in list_user:  \n",
    "        users_sample.append({node_key: node_dict[node_key]})\n",
    "\n",
    "# exemple of user \n",
    "users_sample[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.ii.a - Embeddings on Graph Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we need the associated nodes ids to the sample we definded : \n",
    "node_sample_ids = []\n",
    "for i in range(len(users_sample)):\n",
    "    for key, _ in users_sample[i].items():\n",
    "        node_sample_ids.append(key)\n",
    "\n",
    "# define the subgraph with the sample of 100 nodes\n",
    "sample_graph = g.subgraph(node_sample_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 10791.43it/s]\n",
      "Generating walks (CPU: 1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 158.75it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# we can then build the embeddings of the sampled graph with Node2Vec\n",
    "node2vec = Node2Vec(sample_graph, dimensions=50)\n",
    "fitted_model_n2v = node2vec.fit(window=10, min_count=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have access to the 10 most similar nodes of a node, based on cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('n75904', 0.9822466969490051),\n",
       " ('n70567', 0.9814179539680481),\n",
       " ('n63697', 0.9792734384536743),\n",
       " ('n96427', 0.979026198387146),\n",
       " ('n89546', 0.978333055973053),\n",
       " ('n96452', 0.9771556854248047),\n",
       " ('n80314', 0.9757769703865051),\n",
       " ('n96155', 0.9748070240020752),\n",
       " ('n58317', 0.9738568663597107),\n",
       " ('n96126', 0.9738232493400574)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_model_n2v.wv.most_similar('n96392')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can build a cosine similarity matrix :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the embeddings for every node\n",
    "node_embeddings_n2v = {node: fitted_model_n2v.wv[node] for node in sample_graph.nodes()}\n",
    "\n",
    "# list element is easier to handle\n",
    "list_of_embeddings_n2v = []\n",
    "for key, value in node_embeddings_n2v.items():\n",
    "    list_of_embeddings_n2v.append({key : value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the cosine similarity matrix\n",
    "cos_sim_n2v = [[0 for _ in range(100)] for _ in range(100)]\n",
    "\n",
    "for i, emb_i_dict in enumerate(list_of_embeddings_n2v):\n",
    "    for j, emb_j_dict in enumerate(list_of_embeddings_n2v):\n",
    "        emb_i = next(iter(emb_i_dict.values())) \n",
    "        emb_j = next(iter(emb_j_dict.values()))\n",
    "\n",
    "        cosine_sim = cosine_similarity([emb_i], [emb_j])[0][0]\n",
    "        cos_sim_n2v[i][j] = cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most similar pairs of users are :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(28, 60, 0.997758),\n",
       " (28, 32, 0.9970872),\n",
       " (32, 60, 0.99693966),\n",
       " (2, 28, 0.9957464),\n",
       " (54, 76, 0.995324),\n",
       " (20, 28, 0.99503094),\n",
       " (2, 60, 0.9949315),\n",
       " (2, 20, 0.9948659),\n",
       " (2, 32, 0.9945479),\n",
       " (20, 60, 0.99374366)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from this, we can get the more similar users in our sample graph :\n",
    "arr_cos_sim_n2v = np.array(cos_sim_n2v)\n",
    "np.fill_diagonal(arr_cos_sim_n2v, -np.inf) # we replace the 1 elements of the diagonal by -inf\n",
    "\n",
    "# 10 most similar nodes\n",
    "v = []\n",
    "\n",
    "nb_users_to_print = 10\n",
    "for _ in range(nb_users_to_print):\n",
    "    max_index = np.argmax(arr_cos_sim_n2v)\n",
    "    max_sim = np.max(arr_cos_sim_n2v)\n",
    "\n",
    "    max_row_index, max_col_index = np.unravel_index(max_index, np.array(arr_cos_sim_n2v).shape)\n",
    "\n",
    "    arr_cos_sim_n2v[max_row_index][max_col_index] = -np.inf\n",
    "    arr_cos_sim_n2v[max_col_index][max_row_index] = -np.inf\n",
    "\n",
    "    v.append((max_row_index, max_col_index, max_sim))\n",
    "\n",
    "print(f\"The {nb_users_to_print} most similar pairs of users are :\")\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.ii.b - Embeddings on Post Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n21102': {'labels': ':Tweet',\n",
       "  'is_quote_status': True,\n",
       "  'possibly_sensitive': False,\n",
       "  'retweet_count': 0,\n",
       "  'favorite_count': 0,\n",
       "  'id_str': '592834972265934849',\n",
       "  'isTruncated': False,\n",
       "  'annotation_postPriority': 'Low',\n",
       "  'created_at': '2015-04-27T00:00Z',\n",
       "  'id': '592834972265934849',\n",
       "  'annotation_annotated': True,\n",
       "  'annotation_num_judgements': 1,\n",
       "  'text': 'clever... https://t.co/kl6DVrZX0w',\n",
       "  'topic': 'TRECIS-CTIT-H-019'}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we already have our sampled graph and the users, we now need the tweets\n",
    "tweets = []\n",
    "for node, data in g.nodes(data=True):\n",
    "    for key, value in data.items():\n",
    "        if key == 'labels':\n",
    "            if value == ':Tweet':\n",
    "                tweets.append({node : data})\n",
    "\n",
    "# exemple of tweet\n",
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user': 'n96392',\n",
       "  'tweets': ['These @USAID search and rescue dogs are heading to #Nepal. How you can help #NepalQuakeRelief: http://t.co/XgKnjvvNup http://t.co/JoPISpNIr4']},\n",
       " {'user': 'n64670',\n",
       "  'tweets': ['PDRRMC said that eight bodies had already been recovered from the debris of the supermarket that was torn down by the earthquake in Pampanga, while at least 20 others had been dug up alive but with injuries, including a woman whose leg had to be amputated. | via Ding Cervantes https://t.co/no0xTEAgCx',\n",
       "   'The Philippine Institute of Volcanology and Seismology dismissed rumors saying that a magnitude 7.1 earthquake may hit Metro Manila soon. https://t.co/XifnFpQhOy',\n",
       "   'The entire Pampanga province is now in a state of calamity amid ruins and deaths caused by the 6.1 magnitude earthquake on Monday afternoon. | via Ding Cervantes https://t.co/TWz49BYz9A',\n",
       "   'The Department of Education has ordered a thorough inspection of all school buildings and facilities after a magnitude 6.1 earthquake struck Metro Manila and parts of Luzon on Monday afternoon. | via @jvrmateoSTAR https://t.co/aOv4fWdEdK',\n",
       "   'Netizen Zhaun Rosales Ortega said people are running out of fear of tsunami in Tacloban City on Tuesday. \\\\n\\\\nPhivolcs said there is NO TSUNAMI THREAT from the Eastern Samar quake. (Video courtesy of Zhaun Rosales Ortega) https://t.co/AUf6eE2ks0',\n",
       "   'SOJ Guevarra says buildings in DOJ main office in Manila structurally sound after strong earthquake last Monday | @edupunay',\n",
       "   'Mr. Chu, the owner of a supermarket in Porac, Pampanga which collapsed during a massive earthquake on Monday, has surrendered, PNP chief Gen. Oscar Albayalde said. | via Manny Tupas https://t.co/5w88NY2iF5',\n",
       "   'USGS records magnitude 6.3 quake in Gutad, Floridablanca, Pampanga at 5:11PM on Monday. | @USGS https://t.co/sKuwoSQy98',\n",
       "   'Residents camp out near Iquique after a second earthquake measuring 7.6 rocks northern Chile | PHOTO via AFP http://t.co/HnGqwl2i72',\n",
       "   \"TODAY'S EDITORIAL: Hundreds of aftershocks and a magnitude 6.5 earthquake that shook Eastern Visayas forced the suspension of rescue operations at the Chuzon Supermarket in Porac, Pampanga. https://t.co/s1dlBE5rHY https://t.co/K7uxf4M2Sk\",\n",
       "   'Korean actor Park Bo Gum tweeted on Wednesday that he is postponing his fan meeting scheduled for April 27, 2019 in Manila after the country was hit by strong earthquakes in two consecutive days. https://t.co/Iq3dieLksk https://t.co/F81rzVhkIt',\n",
       "   'Employees at the Senate scurried out of the building after a strong earthquake was felt in Metro Manila this afternoon. https://t.co/dA6LAIXCC3',\n",
       "   'JUST IN: An earthquake was felt here in Manila at 2:03AM. Did you feel it in your area, too? Stay safe. #Lindol #EarthquakePH',\n",
       "   'WATCH: Few hours after the earthquake, a fire breaks out at a residential area in Barangay 5, Leveriza, Malate, Manila on Monday evening. | Video courtesy of Kobe Mallorca https://t.co/UZQyFgYhdw',\n",
       "   'Water flows from the Mezza II Residences in Quezon City after a strong earthquake hit Luzon areas, including Metro Manila on Monday. (Video courtesy of Rea Buenavista) https://t.co/ZIx8VqUPU3',\n",
       "   'Residents of a condominium in QC flock downstairs after an earthquake was felt in Metro Manila this afternoon. | @mjaysoncayabyab https://t.co/Y9JgaKmBk1',\n",
       "   \"Here's what you need to know about the magnitude 6.1 earthquake that hit several parts of Luzon on Monday afternoon. #Lindol #EarthquakePH https://t.co/W9Zn8Mgm45\",\n",
       "   '@DepEd_PH @jvrmateoSTAR LOOK: Schools in Pampanga sustain damage following Monday afternoon?s earthquake. (?DepEd) | @jvrmateoSTAR https://t.co/8GgjeM62YA',\n",
       "   'BEFORE AND AFTER\\\\n\\\\nHere are the before and after photos of the Chuzon Supermarket in Porac, Pampanga where rescuers rush to find more survivors after a magnitude 6.1 quake struck parts of Luzon on Monday. https://t.co/p0bh3aGdH7',\n",
       "   'Porac, Pampanga is now in a state of calamity amid ruins and deaths caused by the 6.1 magnitude earthquake Monday afternoon. | via Ding Cervantes https://t.co/nerw9FFCeG',\n",
       "   'Aid reaches quake-hit Nepal villagers as death toll passes 5,000 http://t.co/j6oOaNt9Jg | via AFP']}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build a list containing dictionnaries with the user_node_id and every tweet made by this user\n",
    "users_posts = []\n",
    "\n",
    "for u in users_sample:\n",
    "    user_node_id = [key for key, _ in u.items()][0]\n",
    "    tweets_by_user = []\n",
    "    \n",
    "    for t in tweets:\n",
    "        tweet_node_id = [key for key, _ in t.items()][0]\n",
    "        text_tweet = [value for _, value in t.items()][0]['text']\n",
    "\n",
    "        if tweet_node_id in g[user_node_id]:\n",
    "            tweets_by_user.append(text_tweet)\n",
    "\n",
    "    users_posts.append({'user':user_node_id,\n",
    "                        'tweets':tweets_by_user})\n",
    "    \n",
    "# exemple of users posts :\n",
    "users_posts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us build the empbeddings of our tweets, using a pretrained Word2Vec model on twitter data, using embeddings of length 50 to match the graph structure embeddings\n",
    "w2v = gensim.downloader.load('glove-twitter-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define a function that allows us to tokenize a tweet/sentence, and we take the average of each embedding\n",
    "def sentence_embedding(sentence):\n",
    "    if len(sentence)>0:\n",
    "        tokens = sentence.split()\n",
    "        embeddings = [w2v[token] for token in tokens if token in w2v]\n",
    "        \n",
    "        if len(tokens) > 0 and embeddings:\n",
    "                avg_embedding = np.mean(embeddings, axis=0)\n",
    "        else:\n",
    "            avg_embedding = np.zeros(w2v.vector_size)\n",
    "    else:\n",
    "         avg_embedding = np.zeros(w2v.vector_size)\n",
    "    return avg_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we then compute and add the embedded tweets to the users_posts list, by adding a key to each dictionnary in the list\n",
    "for i in users_posts:\n",
    "    tweets = i['tweets']\n",
    "    embedded_tweets = []\n",
    "\n",
    "    if len(tweets) == 0:\n",
    "        avg_emb_tweet = np.zeros(w2v.vector_size)\n",
    "\n",
    "    else :\n",
    "        for j in i['tweets']:\n",
    "            emb_tweet = sentence_embedding(j)\n",
    "            embedded_tweets.append(emb_tweet)\n",
    "            avg_emb_tweet = np.mean(embedded_tweets, axis=0)\n",
    "    i['embedded_tweets'] = avg_emb_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user': 'n96392',\n",
       " 'tweets': ['These @USAID search and rescue dogs are heading to #Nepal. How you can help #NepalQuakeRelief: http://t.co/XgKnjvvNup http://t.co/JoPISpNIr4'],\n",
       " 'embedded_tweets': array([ 1.0964541e-01,  3.8855222e-01, -4.4583836e-01, -4.6119708e-01,\n",
       "        -2.1502979e-01, -1.4820620e-01,  8.6893910e-01, -1.5973497e-02,\n",
       "        -3.8214959e-03, -3.9347604e-01, -1.0693520e-01, -3.1848807e-02,\n",
       "        -4.8738604e+00, -8.9479998e-02,  2.5382513e-02, -3.4874976e-03,\n",
       "        -8.6597502e-02, -5.6521899e-01, -1.8982593e-02, -4.0906811e-01,\n",
       "         1.3463980e-01, -1.3434881e-01, -2.9168600e-01,  3.1854680e-01,\n",
       "        -2.5954548e-01,  4.4522700e-01,  2.2805188e-02,  4.2455134e-01,\n",
       "         6.7810601e-01,  9.6944995e-02,  1.9368540e-01, -2.1784198e-02,\n",
       "        -6.7391307e-03,  1.2858097e-01,  4.7495198e-01, -1.7855896e-01,\n",
       "         1.8710601e-01,  1.2585229e-01, -1.7249830e-01, -4.7245007e-02,\n",
       "        -6.7430609e-01, -5.6812032e-03,  1.1368439e-01,  1.7586289e-01,\n",
       "         4.1750631e-01, -1.5007800e-01,  7.7397139e-03,  1.5162079e-01,\n",
       "         9.5115393e-02,  6.7266308e-02], dtype=float32)}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exemple of new user post :\n",
    "users_posts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now compute the cosine similarity matrix like with our graph structure embeddings\n",
    "cos_sim_w2v = [[0 for _ in range(100)] for _ in range(100)]\n",
    "\n",
    "for i in range(len(users_posts)):\n",
    "    for j in range(len(users_posts)):\n",
    "        if i == j:\n",
    "            cos_sim_w2v[i][j] = 1\n",
    "        else:\n",
    "            emb_i = users_posts[i]['embedded_tweets'].reshape(1, -1)\n",
    "            emb_j = users_posts[j]['embedded_tweets'].reshape(1, -1)\n",
    "            cos_sim_w2v[i][j] = cosine_similarity(emb_i, emb_j)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 more similar pairs of users are :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 25, 0.9969907402992249),\n",
       " (1, 32, 0.9966518878936768),\n",
       " (25, 32, 0.996433436870575),\n",
       " (63, 70, 0.9958035945892334),\n",
       " (32, 70, 0.9955213069915771),\n",
       " (8, 62, 0.9954821693926238),\n",
       " (1, 8, 0.9953297365470238),\n",
       " (12, 25, 0.9948004484176636),\n",
       " (1, 62, 0.9947628974914551),\n",
       " (25, 62, 0.9947424530982971)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finally, we can print the most similar users based on this matrix\n",
    "arr_cos_sim_w2v = np.array(cos_sim_w2v)\n",
    "np.fill_diagonal(arr_cos_sim_w2v, -np.inf) # replace the diagonal full of 1s by -inf \n",
    "\n",
    "v = []\n",
    "\n",
    "nb_users_to_print = 10\n",
    "for _ in range(nb_users_to_print):\n",
    "    max_index = np.argmax(arr_cos_sim_w2v)\n",
    "    max_sim = np.max(arr_cos_sim_w2v)\n",
    "\n",
    "    max_row_index, max_col_index = np.unravel_index(max_index, np.array(arr_cos_sim_w2v).shape)\n",
    "\n",
    "    arr_cos_sim_w2v[max_row_index][max_col_index] = -np.inf\n",
    "    arr_cos_sim_w2v[max_col_index][max_row_index] = -np.inf\n",
    "    \n",
    "    v.append((max_row_index, max_col_index, max_sim))\n",
    "\n",
    "print(f\"The {nb_users_to_print} more similar pairs of users are :\")\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nepal Govt thanks @PMOIndia for going all out to help with post-earthquake story https://t.co/xEBdCeaCLJ', 'Starts to rain in Kathmandu. Rescue operations affected. Officials fear wet bodies will start rotting, stinking. Fear of mass epidemic.', 'Nepal begins 3 days of mourning to condole loss of lives caused by earthquake. Flag at half mast across nation. PM Koirala thanks @PMOIndia', 'The fearless Gorkha. Respect. http://t.co/KcMwZIIw8p']\n",
      "\n",
      "['Reduced cost of calls to #Nepal to 1ï¿½/min (from 19ï¿½/min) to help loved ones connect? http://t.co/RjfHY8bCri @GoogleCR http://t.co/fWEb70Pp8P', 'People worldwide are looking for ways to help #Nepal. Searches visualized: http://t.co/dWBufW30yP via @GoogleTrends http://t.co/ZkD7lycTnb', 'Updated satellite imagery of #Nepal. Here, a busy intersection and temple, before and after ? http://t.co/RjfHY8bCri http://t.co/nyY3b1ADy9']\n"
     ]
    }
   ],
   "source": [
    "# let us look at users 71 and 92 posts (post 57 is very long so bit hard to analyze)\n",
    "print(users_posts[4]['tweets'])\n",
    "print()\n",
    "print(users_posts[5]['tweets'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the earthquake subgraph for example, it seems that both tweets are talking mainly about events that happened in Nepal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.iii - Trends in Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us import a pretrained Word2Vec model that we will use in the looped following function \n",
    "w2v = gensim.downloader.load('glove-twitter-50')\n",
    "\n",
    "def similarity_matrix(selected_graph):\n",
    "\n",
    "    path = f'subgraphs_data/{chosen_event}_subgraph.graphml'\n",
    "    g = nx.read_graphml(path)\n",
    "\n",
    "    users = []\n",
    "    nodes_users = []\n",
    "    for node, data in g.nodes(data=True):\n",
    "        for key, value in data.items():\n",
    "            if key == 'labels':\n",
    "                if value == ':User':\n",
    "                    nodes_users.append(node)\n",
    "                    users.append({node : data})\n",
    "\n",
    "    #We choose the size of the sample\n",
    "    n=100\n",
    "\n",
    "    # Sample n/2 users with highest degree centrality\n",
    "    degree_centralities = nx.degree_centrality(g) #We first compute the degree coef fo all nodes\n",
    "    degree_filtred = {key: value for key, value in degree_centralities.items() if key in nodes_users} #We then only select the users nodes\n",
    "    top_degree_users = sorted(degree_filtred, key=lambda x: degree_filtred[x], reverse=True)[0:int(n/2)] #We take the 50 users that have the highest degree centrality\n",
    "\n",
    "\n",
    "    filtered_nodes = [node for node in nodes_users if node not in top_degree_users] #We create a list of all the users except the ones already in the top 50 of degree centrality\n",
    "\n",
    "    # Sample n/2 users with highest closeness centrality (taking away the nodes that are already in the highest degree)\n",
    "    closeness_centralities = nx.closeness_centrality(g) #We first compute the closeness coef fo all nodes\n",
    "    closeness_filtred = {key: value for key, value in closeness_centralities.items() if key in filtered_nodes} #We then only select the users nodes (that are not in the top 50 degree centrality)\n",
    "    top_closeness_users = sorted(closeness_filtred, key=lambda x: closeness_filtred[x], reverse=True)[0:int(n/2)]  #We take the 50 users that have the highest closeness centrality\n",
    "\n",
    "    # List of the selected users\n",
    "    list_user = list(set(top_degree_users + top_closeness_users))\n",
    "\n",
    "    users_sample = []\n",
    "\n",
    "    for node_dict in users:\n",
    "        node_key = list(node_dict.keys())[0]  \n",
    "        if node_key in list_user:  \n",
    "            users_sample.append({node_key: node_dict[node_key]})\n",
    "\n",
    "\n",
    "    # first, we need the associated nodes ids to the sample we definded : \n",
    "    node_sample_ids = []\n",
    "    for i in range(len(users_sample)):\n",
    "        for key, _ in users_sample[i].items():\n",
    "            node_sample_ids.append(key)\n",
    "\n",
    "    # define the subgraph with the sample of 100 nodes\n",
    "    sample_graph = g.subgraph(node_sample_ids)\n",
    "\n",
    "    # we can then build the embeddings of the sampled graph with Node2Vec\n",
    "    node2vec = Node2Vec(sample_graph, dimensions=50)\n",
    "    fitted_model_n2v = node2vec.fit(window=10, min_count=1) \n",
    "    \n",
    "    # get the embeddings for every node\n",
    "    node_embeddings_n2v = {node: fitted_model_n2v.wv[node] for node in sample_graph.nodes()}\n",
    "\n",
    "    # list element is easier to handle\n",
    "    list_of_embeddings_n2v = []\n",
    "    for key, value in node_embeddings_n2v.items():\n",
    "        list_of_embeddings_n2v.append({key : value})\n",
    "\n",
    "    # build the cosine similarity matrix\n",
    "    cos_sim_n2v = [[0 for _ in range(100)] for _ in range(100)]\n",
    "\n",
    "    for i, emb_i_dict in enumerate(list_of_embeddings_n2v):\n",
    "        for j, emb_j_dict in enumerate(list_of_embeddings_n2v):\n",
    "            emb_i = next(iter(emb_i_dict.values())) \n",
    "            emb_j = next(iter(emb_j_dict.values()))\n",
    "\n",
    "            cosine_sim = cosine_similarity([emb_i], [emb_j])[0][0]\n",
    "            cos_sim_n2v[i][j] = cosine_sim\n",
    "\n",
    "\n",
    "    # we already have our sampled graph and the users, we now need the tweets\n",
    "    tweets = []\n",
    "    for node, data in g.nodes(data=True):\n",
    "        for key, value in data.items():\n",
    "            if key == 'labels':\n",
    "                if value == ':Tweet':\n",
    "                    tweets.append({node : data})\n",
    "\n",
    "\n",
    "    # build a list containing dictionnaries with the user_node_id and every tweet made by this user\n",
    "    users_posts = []\n",
    "\n",
    "    for u in users_sample:\n",
    "        user_node_id = [key for key, _ in u.items()][0]\n",
    "        tweets_by_user = []\n",
    "        \n",
    "        for t in tweets:\n",
    "            tweet_node_id = [key for key, _ in t.items()][0]\n",
    "            text_tweet = [value for _, value in t.items()][0]['text']\n",
    "\n",
    "            if tweet_node_id in g[user_node_id]:\n",
    "                tweets_by_user.append(text_tweet)\n",
    "\n",
    "        users_posts.append({'user':user_node_id,\n",
    "                            'tweets':tweets_by_user})    \n",
    "\n",
    "\n",
    "    # we then compute and add the embedded tweets to the users_posts list, by adding a key to each dictionnary in the list\n",
    "    for i in users_posts:\n",
    "        tweets = i['tweets']\n",
    "        embedded_tweets = []\n",
    "\n",
    "        if len(tweets) == 0:\n",
    "            avg_emb_tweet = np.zeros(w2v.vector_size)\n",
    "\n",
    "        else :\n",
    "            for j in i['tweets']:\n",
    "                emb_tweet = sentence_embedding(j)\n",
    "                embedded_tweets.append(emb_tweet)\n",
    "                avg_emb_tweet = np.mean(embedded_tweets, axis=0)\n",
    "        i['embedded_tweets'] = avg_emb_tweet\n",
    "\n",
    "    # we can now compute the cosine similarity matrix like with our graph structure embeddings\n",
    "    cos_sim_w2v = [[0 for _ in range(100)] for _ in range(100)]\n",
    "\n",
    "    for i in range(len(users_posts)):\n",
    "        for j in range(len(users_posts)):\n",
    "            if i == j:\n",
    "                cos_sim_w2v[i][j] = 1\n",
    "            else:\n",
    "                emb_i = users_posts[i]['embedded_tweets'].reshape(1, -1)\n",
    "                emb_j = users_posts[j]['embedded_tweets'].reshape(1, -1)\n",
    "                cos_sim_w2v[i][j] = cosine_similarity(emb_i, emb_j)[0][0]\n",
    "    return([cos_sim_n2v, cos_sim_w2v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 13723.92it/s]\n",
      "Generating walks (CPU: 1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 161.24it/s]\n",
      "Computing transition probabilities: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 12394.52it/s]\n",
      "Generating walks (CPU: 1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 119.93it/s]\n",
      "Computing transition probabilities: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 16804.10it/s]\n",
      "Generating walks (CPU: 1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 200.57it/s]\n",
      "Computing transition probabilities: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 16707.71it/s]\n",
      "Generating walks (CPU: 1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 214.65it/s]\n",
      "Computing transition probabilities: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 19916.92it/s]\n",
      "Generating walks (CPU: 1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 217.86it/s]\n",
      "Computing transition probabilities: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 15819.80it/s]\n",
      "Generating walks (CPU: 1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 219.62it/s]\n"
     ]
    }
   ],
   "source": [
    "correlation_results = []\n",
    "\n",
    "events = ['wildfire', 'earthquake', 'typhoon', 'bombing', 'flood', 'shooting']\n",
    "\n",
    "for event_type in events:\n",
    "\n",
    "    res = similarity_matrix(event_type)\n",
    "    \n",
    "    graph_similarities = [similarity for sublist in res[0] for similarity in sublist]\n",
    "    content_similarities = [similarity for sublist in res[1] for similarity in sublist]\n",
    "\n",
    "    # Compute the correlation between the graph structure and tweet content similarities\n",
    "    correlation, p_value = spearmanr(graph_similarities, content_similarities)\n",
    "    \n",
    "    # Append the correlation coefficient and p-value to the results list\n",
    "    correlation_results.append({'event_type': event_type, 'correlation': correlation, 'p_value': p_value})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'event_type': 'wildfire',\n",
       "  'correlation': 0.12185115780741214,\n",
       "  'p_value': 2.1710614502036913e-34},\n",
       " {'event_type': 'earthquake',\n",
       "  'correlation': 0.19815285054191645,\n",
       "  'p_value': 4.3690508417981185e-89},\n",
       " {'event_type': 'typhoon',\n",
       "  'correlation': 0.11543529291262557,\n",
       "  'p_value': 5.155231672948199e-31},\n",
       " {'event_type': 'bombing',\n",
       "  'correlation': 0.15581431825329534,\n",
       "  'p_value': 2.2313674210928092e-55},\n",
       " {'event_type': 'flood',\n",
       "  'correlation': 0.16812255257340036,\n",
       "  'p_value': 2.6675657379862692e-64},\n",
       " {'event_type': 'shooting',\n",
       "  'correlation': 0.18783889173316048,\n",
       "  'p_value': 4.3777891854332924e-80}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this results, we see that the similarity matrices for the graph structure and post contents are more correlated for the flood and shooting event (higher p-value) than for the earthquake and typhoon events. Moreover we see that in general we observe a positive correlation between the two matrices of similarity except for the shooting event."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
