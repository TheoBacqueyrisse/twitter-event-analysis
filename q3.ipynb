{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/python/3.10.13/lib/python3.10/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/codespace/.local/lib/python3.10/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/codespace/.local/lib/python3.10/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from gensim) (7.0.3)\n",
      "Requirement already satisfied: wrapt in /usr/local/python/3.10.13/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in /usr/local/python/3.10.13/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk) (4.66.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "%pip install gensim\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "%pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the graph \n",
    "path = 'database_formated_for_NetworkX.graphml'\n",
    "g = nx.read_graphml(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n56568': {'labels': ':User', 'statuses_count': 13211, 'favourites_count': 196, 'isVerified': False, 'screen_name': 'jeffseroka', 'followers_count': 537, 'listed_count': 3, 'name': 'Jeff', 'tweets_count': 1, 'id': '54549327', 'friends_count': 400}}\n",
      "{'n583': 'RT @northfortynews: Tanker helicopter heads up to Paradise Park to drop water on #HighParkFire. http://t.co/7atRS5cy'}\n"
     ]
    }
   ],
   "source": [
    "# retrieve the users of our graph\n",
    "users = []\n",
    "nodes_users = []\n",
    "for node, data in g.nodes(data=True):\n",
    "    for key, value in data.items():\n",
    "        if key == 'labels':\n",
    "            if value == ':User':\n",
    "                nodes_users.append(node)\n",
    "                users.append({node : data})\n",
    "\n",
    "# exemple of user\n",
    "print(users[0])\n",
    "\n",
    "# retrieve the tweets of our graph\n",
    "tweets = []\n",
    "for node, data in g.nodes(data=True):\n",
    "    for key, value in data.items():\n",
    "        if key == 'labels':\n",
    "            if value == ':Tweet':\n",
    "                tweets.append({node : data['text']})\n",
    "\n",
    "# exemple of tweet\n",
    "print(tweets[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us build a list containing the Poster of a tweet, his tweet, and the degree centrality of the poster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_and_poster = []\n",
    "list_centralities = nx.degree_centrality(g)\n",
    "\n",
    "for t in tweets:\n",
    "    for key, value in t.items():\n",
    "        tweet_id = key\n",
    "    neighb = list(g.neighbors(tweet_id))\n",
    "    for n in neighb:\n",
    "        if g.nodes[n]['labels'] == ':User':\n",
    "            tweets_and_poster.append({'poster' : n,\n",
    "                                      'tweet' : value,\n",
    "                                      'deg_centrality' : list_centralities[n]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import english stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# function to remove the stopwords from a tweet\n",
    "def remove_stopwords(tweet):\n",
    "    words = tweet.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'poster': 'n97153',\n",
       " 'tweet': 'RT @northfortynews: Tanker helicopter heads Paradise Park drop water #HighParkFire. http://t.co/7atRS5cy',\n",
       " 'deg_centrality': 3.6487694525021436e-05}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stopwords from all the tweets that we consider\n",
    "for t in tweets_and_poster:\n",
    "    t['tweet'] = remove_stopwords(t['tweet'])\n",
    "\n",
    "# exemple of tweets without stopwords\n",
    "tweets_and_poster[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us build a function to get the embeddings of each tweet, weighted by the poster degree centrality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained Word2Vec model\n",
    "w2v = gensim.downloader.load('glove-twitter-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_embeddings(tweet_and_poster):\n",
    "    tweet = tweet_and_poster['tweet']\n",
    "    if len(tweet)>0:\n",
    "        tokens = tweet.split()\n",
    "        embeddings = [w2v[token] for token in tokens if token in w2v]\n",
    "        \n",
    "        if len(tokens) > 0 and embeddings:\n",
    "                avg_embedding = np.mean(embeddings, axis=0)\n",
    "        else:\n",
    "            avg_embedding = np.zeros(w2v.vector_size)\n",
    "    else:\n",
    "         avg_embedding = np.zeros(w2v.vector_size)\n",
    "\n",
    "    # muliply the embedding vector by the poster degree centrality\n",
    "    avg_embedding *= tweet_and_poster['deg_centrality']\n",
    "    return avg_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'poster': 'n97153',\n",
       " 'tweet': 'RT @northfortynews: Tanker helicopter heads Paradise Park drop water #HighParkFire. http://t.co/7atRS5cy',\n",
       " 'deg_centrality': 3.6487694525021436e-05,\n",
       " 'weighted_embedding': array([-1.5352563e-05,  7.3326587e-06,  8.6894534e-06, -1.1866437e-05,\n",
       "        -3.6320762e-06,  7.6861324e-06,  1.2904969e-05, -1.3077737e-05,\n",
       "         1.7441947e-05, -1.0066772e-05,  9.8510382e-06,  1.2501506e-05,\n",
       "        -1.3010234e-04, -5.6111962e-06,  1.3717749e-05, -7.6460438e-08,\n",
       "        -4.5607148e-06, -8.2355555e-06, -6.5395091e-07, -1.9738109e-05,\n",
       "        -2.4089268e-05, -6.0406201e-06,  1.3080290e-05, -1.9026488e-06,\n",
       "         9.9687668e-06,  2.1328060e-05, -2.2642258e-05,  1.1495822e-05,\n",
       "        -1.3339501e-05,  1.0603259e-05, -5.0288395e-06,  1.5014541e-05,\n",
       "         1.4268059e-06,  1.1140606e-06,  2.3357870e-05, -2.6931195e-06,\n",
       "        -3.5430830e-06,  1.3394040e-05,  1.6297501e-05,  6.8883296e-06,\n",
       "        -5.2485721e-06, -1.2152229e-06, -2.3539216e-05, -4.2540187e-06,\n",
       "         9.4786365e-06,  5.5036758e-06,  3.8331236e-06,  1.1297684e-05,\n",
       "         1.4776559e-05,  3.5481818e-05], dtype=float32)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the weighted embedding key to the tweet and poster list of dictionnaries\n",
    "for t in tweets_and_poster:\n",
    "    t['weighted_embedding'] = weighted_embeddings(t)\n",
    "\n",
    "# exemple of embedding\n",
    "tweets_and_poster[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us build the top-k tweets retrieval of a query\n",
    "\n",
    "- Since the query is supposed to be built of keywords, it should not contain stopwords\n",
    "\n",
    "- If we compute the cosine similarity of the embedded query and all the embeddings, we can retrieve the top-k more relevant tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_retrieval(keywords, k):\n",
    "    print(keywords)\n",
    "    print() \n",
    "\n",
    "    embedding = [w2v[token] for token in keywords if token in w2v]\n",
    "    embedding = np.mean(embedding, axis=0).reshape(1, -1)\n",
    "\n",
    "    similarity_list = [cosine_similarity(embedding, i['weighted_embedding'].reshape(1, -1)) for i in tweets_and_poster]\n",
    "    \n",
    "    related_tweets = [i['tweet'] for i in tweets_and_poster]\n",
    "\n",
    "    sorted_similarity_list = sorted(zip(similarity_list, related_tweets), reverse = True)\n",
    "    sorted_sims, sorted_tweets = zip(*sorted_similarity_list)\n",
    "\n",
    "    for i in range(len(sorted_sims[:k])):\n",
    "        print(f'Cosine Sim : {sorted_sims[i][0][0]} \\n Tweet : {sorted_tweets[i]} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['air', 'injury', 'help', 'fire', 'congratulations']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Sim : 0.9344030618667603 \n",
      " Tweet : UPDATE: #CycloneKenneth made landfall #Mozambique. 700,000 people live cyclone?s path least three lost lives. @WFP ground preparing emergency response. See help here: https://t.co/NUSL0GBG8w https://t.co/fZWm0it37c \n",
      "\n",
      "Cosine Sim : 0.9302331805229187 \n",
      " Tweet : staff live #Tacloban 7pm @BBCNews city braces battering #TyphoonHagupit - one year devastating #Haiyan \n",
      "\n",
      "Cosine Sim : 0.9292325973510742 \n",
      " Tweet : Premier @RachelNotley thanks fire crews working fight fire help residents affected #ymmfire \n",
      "\n",
      "Cosine Sim : 0.9281186461448669 \n",
      " Tweet : RT @emitoms: \"There reports Boston Marathon runners crossing finish line continuing run Hospital give blood v ... \n",
      "\n",
      "Cosine Sim : 0.9281186461448669 \n",
      " Tweet : RT @emitoms: \"There reports Boston Marathon runners crossing finish line continuing run Hospital give blood v ... \n",
      "\n",
      "Cosine Sim : 0.927777886390686 \n",
      " Tweet : @anupdgn : Government trying best speed rescue operation Bhaktapur \n",
      "\n",
      "Cosine Sim : 0.9265118837356567 \n",
      " Tweet : RT @ReBourne_Again: Everyone please pray state Colorado! 9 fires burning state! recent \"The... h ... \n",
      "\n",
      "Cosine Sim : 0.9260876774787903 \n",
      " Tweet : RT @cgtnamerica: UPDATE: least 20 injured mass shooting Parkland, FLA high school; SWAT emergency rescue present shooter? \n",
      "\n",
      "Cosine Sim : 0.924542248249054 \n",
      " Tweet : Additional staff #Ontario?s Aviation, Forest Fire Emergency Services headed west assist @AlbertaWildfire colleagues. #MNRF also sending equipment pumps, hose &amp; hand tools support Alberta time escalated fire activity. #ABFire https://t.co/LBPvzwY1n7 \n",
      "\n",
      "Cosine Sim : 0.92377108335495 \n",
      " Tweet : RT @trevortombe: Complaints today (of days) gov't \"cutting fire budget\" stupid. It'll cost costs. Budget cause? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = ['air', 'injury', 'help', 'fire', 'congratulations']\n",
    "k = 10\n",
    "\n",
    "top_k_retrieval(query, k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
