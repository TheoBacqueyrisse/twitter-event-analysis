{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEB MINING PROJECT:\n",
    "# Mining Information from Social Media Networks During Crisis Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, let's upload the graph we will be working, the *database_formated_for_NetworkX.graphml* being in the current working directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/database_formated_for_NetworkX.graphml'\n",
    "graph = nx.read_graphml(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Analyses:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('Number of nodes:', graph.number_of_nodes())\n",
    "print('Number of edges:', graph.number_of_edges())\n",
    "print('Directed graph:', nx.is_directed(graph))\n",
    "print('Weighted graph:', any('weight' in data for _, _, data in graph.edges(data=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our multi-layered graph is directed, unweighted and comprised of 109,627 nodes and 311,654 edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look closer to the structure of the nodes and the edges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#About the nodes:\n",
    "\n",
    "i = 0\n",
    "for node, data in graph.nodes(data=True):\n",
    "    i+=1\n",
    "    print(\"Node:\", node)\n",
    "    for key, value in data.items():\n",
    "        print(\"\\t\", key, \":\", value)\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#About the edges:\n",
    "\n",
    "i = 0\n",
    "for edge in graph.edges(data=True):\n",
    "    i+=1\n",
    "    print(\"Edge:\", edge)\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what node labels and event types we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_values = set()\n",
    "event_type_values = set()\n",
    "\n",
    "for node, data in graph.nodes(data=True):\n",
    "    labels_values.update(data.get('labels', '').split(\":\"))\n",
    "    event_type_values.add(data.get('eventType', ''))\n",
    "    \n",
    "print(labels_values)\n",
    "print(event_type_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have nodes of type 'Tweet', 'Event', 'User', 'Hashtag' and 'PostCategory'. We also have 6 seperate events to consider, 'wildfire', 'earthquake', 'shooting', 'typhoon', 'flood' and 'bombing', which will be the attributes behind the construction of our subgraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separation of our graph into event-based subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraphs = {event_type: graph.subgraph([node for node, data in graph.nodes(data=True) if data.get('eventType') == event_type]) \n",
    "             for event_type in ['wildfire', 'earthquake', 'typhoon', 'bombing', 'flood', 'shooting']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary with each event as keys, and id values linking tweets to events as values\n",
    "\n",
    "event_trecisid_mapping = {}\n",
    "events = ['wildfire', 'earthquake', 'typhoon', 'bombing', 'flood', 'shooting']\n",
    "\n",
    "for event_type in events:\n",
    "    trecisid_values = []\n",
    "    \n",
    "    #Iterate over the nodes of the different event type and collect their 'trecisid' values\n",
    "    for node, data in subgraphs[event_type].nodes(data=True):\n",
    "        if 'trecisid' in data:\n",
    "            trecisid_values.append(data['trecisid'])\n",
    "    \n",
    "    #Add the event type and its corresponding 'trecisid' values to the dictionary\n",
    "    event_trecisid_mapping[event_type] = trecisid_values\n",
    "    \n",
    "event_trecisid_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary of events and their corresponding 'Tweet' nodes:\n",
    "\n",
    "event_tweets = {}\n",
    "\n",
    "for event_type in events:\n",
    "    event_trecisids = event_trecisid_mapping.get(event_type, set())\n",
    "    \n",
    "    event_tweet_list = []\n",
    "    \n",
    "    for node, data in graph.nodes(data=True):\n",
    "        if 'topic' in data:\n",
    "            if data['topic'] in event_trecisids:\n",
    "                event_tweet_list.append(node)\n",
    "    \n",
    "    event_tweets[event_type] = event_tweet_list\n",
    "\n",
    "print(event_tweets['wildfire'][:10])\n",
    "print(event_tweets['bombing'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function that creates the subgraphs we want based on the nodes in event_tweets and the nodes they share edges with in the initial graph\n",
    "\n",
    "def create_subgraph(event, graph):\n",
    "    nodes = event_tweets.get(event, [])\n",
    "    node_list = []\n",
    "    \n",
    "    for edge in graph.edges():\n",
    "\n",
    "        if edge[0] in nodes or edge[1] in nodes:\n",
    "            node_list.append(edge[0])\n",
    "            node_list.append(edge[1])\n",
    "\n",
    "    node_list = list(dict.fromkeys(node_list))\n",
    "\n",
    "    subgraph = graph.subgraph(node_list)\n",
    "    subgraph_name = f\"{event}_subgraph\"\n",
    "    globals()[subgraph_name] = subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the function for all event types\n",
    "\n",
    "for event in events:\n",
    "    create_subgraph(event, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: the nodes of the earthquake subgraph\n",
    "i=0\n",
    "for node, data in wildfire_subgraph.nodes(data=True):\n",
    "    i+=1\n",
    "    print(\"Node:\", node)\n",
    "    for key, value in data.items():\n",
    "        print(\"\\t\", key, \":\", value)\n",
    "    if i>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: the nodes of the wildfire subgraph\n",
    "i = 0\n",
    "for edge in wildfire_subgraph.edges(data=True):\n",
    "    i+=1\n",
    "    print(\"Edge:\", edge)\n",
    "    if i>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wildfire_subgraph.edges()) + len(earthquake_subgraph.edges()) + len(shooting_subgraph.edges()) + len(bombing_subgraph.edges())+ len(flood_subgraph.edges())+ len(typhoon_subgraph.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wildfire_subgraph.nodes()) + len(earthquake_subgraph.nodes()) + len(shooting_subgraph.nodes()) + len(bombing_subgraph.nodes())+ len(flood_subgraph.nodes())+ len(typhoon_subgraph.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraphs = {'wildfire_subgraph': wildfire_subgraph,\n",
    "             'earthquake_subgraph': earthquake_subgraph,\n",
    "             'typhoon_subgraph': typhoon_subgraph,\n",
    "             'bombing_subgraph': bombing_subgraph,\n",
    "             'flood_subgraph': flood_subgraph,\n",
    "             'shooting_subgraph': shooting_subgraph}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 6 seperate subgraphs of our original graph that represent each major event category. For conveniency, We have stored all our subgraphs in a subgraph dictionary called \"subgraphs\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 The Intensity of the Social Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Total number of tweets per subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tweets_dict = {}\n",
    "\n",
    "for subgraph_name, subgraph in subgraphs.items():\n",
    "    total_tweets = sum(1 for node, data in subgraph.nodes(data=True) if data['labels'] == ':Tweet')\n",
    "    total_tweets_dict[subgraph_name] = total_tweets\n",
    "\n",
    "    \n",
    "print(\"Total tweets in each subgraph:\")\n",
    "for subgraph_name, total_tweets in total_tweets_dict.items():\n",
    "    print(f\"{subgraph_name}: {total_tweets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more tweets in the typhoon subgraph (13,981) than any other subgraph. This is followed by the earthquake subgraph (13,056), the shooting subgraph (9,133), the wildfire subgraph (7826) and finally the bombing subgraph (4,186). Twitter users seem to be more inclined to tweet about a typhoon than any over event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract subgraph names and total tweet counts from the dictionary\n",
    "subgraph_names = list(total_tweets_dict.keys())\n",
    "total_tweets_counts = list(total_tweets_dict.values())\n",
    "\n",
    "# Define colors for each subgraph\n",
    "colors = ['orange', 'brown', 'lightblue', 'red', 'blue', 'black']\n",
    "\n",
    "# Plot the bar graph with specified colors\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(subgraph_names, total_tweets_counts, color=colors)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Event Subgraph')\n",
    "plt.ylabel('Total Number of Tweets')\n",
    "plt.title('Total Tweets in Each Subgraph')\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tweets_dict = {}\n",
    "\n",
    "for subgraph_name, subgraph in subgraphs.items():\n",
    "    # Initialize counters for tweets and retweets\n",
    "    total_tweets = 0\n",
    "    total_retweets = 0\n",
    "    \n",
    "    # Iterate over nodes to count tweets\n",
    "    for node, data in subgraph.nodes(data=True):\n",
    "        if data.get('labels') == ':Tweet':\n",
    "            total_tweets += 1\n",
    "            \n",
    "    # Iterate over edges to count retweets\n",
    "    for source, target, data in subgraph.edges(data=True):\n",
    "        if data.get('label') == 'RETWEETS':\n",
    "            total_retweets += data.get('times', 0)\n",
    "    \n",
    "    # Update total_tweets_dict with total counts\n",
    "    total_tweets_dict[subgraph_name] = total_tweets + total_retweets\n",
    "\n",
    "# Print the total tweets (including retweets) in each subgraph\n",
    "print(\"Total tweets (including retweets) in each subgraph:\")\n",
    "for subgraph_name, total_tweets in total_tweets_dict.items():\n",
    "    print(f\"{subgraph_name}: {total_tweets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we include retweets in our tweet count, the ranking established above remains unchanged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract subgraph names and total tweet counts from the dictionary\n",
    "subgraph_names = list(total_tweets_dict.keys())\n",
    "total_tweets_counts = list(total_tweets_dict.values())\n",
    "\n",
    "# Define colors for each subgraph\n",
    "colors = ['orange', 'brown', 'lightblue', 'red', 'blue', 'black']\n",
    "\n",
    "# Plot the bar graph with specified colors\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(subgraph_names, total_tweets_counts, color=colors)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Event Subgraph')\n",
    "plt.ylabel('Total Number of Tweets')\n",
    "plt.title('Total Tweets (including retweets) in Each Subgraph')\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Total followers reached by tweets per subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_followers_dict = {}\n",
    "\n",
    "for subgraph_name, subgraph in subgraphs.items():\n",
    "    total_followers = 0\n",
    "    for tweet_node, user_node, data in subgraph.edges(data=True):\n",
    "        if subgraph.nodes[user_node]['labels'] == ':User':\n",
    "            followers_count = subgraph.nodes[user_node].get('followers_count', 0)\n",
    "            total_followers += followers_count\n",
    "    total_followers_dict[subgraph_name] = total_followers\n",
    "\n",
    "print(\"Total followers reached by tweets in each subgraph:\")\n",
    "for subgraph_name, total_followers in total_followers_dict.items():\n",
    "    print(f\"{subgraph_name}: {total_followers}\")\n",
    "#doesn't consider overlap between followers of some users and followers of others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that tweets in the earthquake subgraph tend to reach the most followers, followed by tweets in the shooting subgraph, the typhoon subgraph, the bombing subgraph, the flood subgraph and finally the wildfire subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph_names = list(total_tweets_dict.keys())\n",
    "total_tweets_counts = list(total_followers_dict.values())\n",
    "\n",
    "# Define colors for each subgraph\n",
    "colors = ['orange', 'brown', 'lightblue', 'red', 'blue', 'black']\n",
    "\n",
    "# Plot the bar graph with specified colors\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(subgraph_names, total_tweets_counts, color=colors)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Event Subgraph')\n",
    "plt.ylabel('Total Number of Tweets')\n",
    "plt.title('Total followers reached by tweets in each subgraph')\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Hashtags per subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = []\n",
    "\n",
    "# Iterate over nodes in the subgraph\n",
    "for node, data in graph.nodes(data=True):\n",
    "    # Check if the node has labels \":Hashtag\"\n",
    "    if data.get('labels') == ':Hashtag':\n",
    "        # Append the hashtag ID to the list\n",
    "        hashtags.append(data['id'])\n",
    "\n",
    "print(len(hashtags))\n",
    "print(len(set(hashtags)))\n",
    "\n",
    "# exemples\n",
    "hashtags[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is 10,441 different subgraphs in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Initialize an empty dictionary to store DataFrames for each subgraph\n",
    "hashtag_dfs = {}\n",
    "\n",
    "# Iterate over each subgraph\n",
    "for subgraph_name, subgraph in subgraphs.items():\n",
    "    # Initialize an empty list to store all hashtag words\n",
    "    hashtag_words = []\n",
    "\n",
    "    # Iterate over nodes in the subgraph\n",
    "    for node, data in subgraph.nodes(data=True):\n",
    "        # Check if the node has labels \":Tweet\"\n",
    "        if data.get('labels') == ':Tweet':\n",
    "            # Get the text of the tweet\n",
    "            tweet_text = data.get('text', '')\n",
    "            # Split the text into words\n",
    "            words = tweet_text.split()\n",
    "            # Extract words starting with \"#\"\n",
    "            hashtag_words.extend([word.strip(\"#\") for word in words if word.startswith(\"#\")])\n",
    "\n",
    "    # Convert the list of hashtag words into a Counter object to count occurrences\n",
    "    hashtag_counts = Counter(hashtag_words)\n",
    "\n",
    "    # Convert the Counter object into a pandas DataFrame\n",
    "    hashtag_df = pd.DataFrame(hashtag_counts.items(), columns=['Hashtag', 'Occurences'])\n",
    "\n",
    "    # Sort the DataFrame by the number of occurrences in descending order\n",
    "    hashtag_df = hashtag_df.sort_values(by='Occurences', ascending=False)\n",
    "\n",
    "    # Reset the index of the DataFrame\n",
    "    hashtag_df = hashtag_df.reset_index(drop=True)\n",
    "\n",
    "    # Store the DataFrame in the dictionary with the subgraph name as the key\n",
    "    hashtag_dfs[subgraph_name] = hashtag_df\n",
    "\n",
    "# Print the DataFrames for each subgraph\n",
    "for subgraph_name, df in hashtag_dfs.items():\n",
    "    print(f\"Exemples of Hashtags for {subgraph_name} subgraph:\")\n",
    "    print(df[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wildfire_hashtags_df = hashtag_dfs['wildfire_subgraph']\n",
    "wildfire_hashtags_df['Subgraph'] = \"Wildfire\"\n",
    "\n",
    "filtered_hashtag_df = wildfire_hashtags_df[wildfire_hashtags_df['Occurences'] >= 20]\n",
    "\n",
    "# Plot the filtered DataFrame as a histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(filtered_hashtag_df['Hashtag'], filtered_hashtag_df['Occurences'], color='skyblue')\n",
    "plt.xlabel('Hashtag')\n",
    "plt.ylabel('Occurences')\n",
    "plt.title('Hashtag Occurrences in the Wildfire Subgraph (Count >= 20)')\n",
    "plt.xticks(rotation=90)  \n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hashtag \"ymmfire\" was by far the most used hashtag in the wildfire subgraph with 2,408 total occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_hashtags_df = hashtag_dfs['earthquake_subgraph']\n",
    "earthquake_hashtags_df['Subgraph'] = \"Earthquake\"\n",
    "\n",
    "filtered_hashtag_df = earthquake_hashtags_df[earthquake_hashtags_df['Occurences'] >= 20]\n",
    "\n",
    "# Plot the filtered DataFrame as a histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(filtered_hashtag_df['Hashtag'], filtered_hashtag_df['Occurences'], color='skyblue')\n",
    "plt.xlabel('Hashtag')\n",
    "plt.ylabel('Occurences')\n",
    "plt.title('Hashtag Occurrences in the Earthquake Subgraph (Count >= 20)')\n",
    "plt.xticks(rotation=90) \n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hashtag \"NepalEarthquake\" was by far the most used hashtag in the earthquake subgraph with 1,154 total occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typhoon_hashtags_df = hashtag_dfs['typhoon_subgraph']\n",
    "typhoon_hashtags_df['Subgraph'] = \"Typhoon\"\n",
    "\n",
    "filtered_hashtag_df = typhoon_hashtags_df[typhoon_hashtags_df['Occurences'] >= 20]\n",
    "\n",
    "# Plot the filtered DataFrame as a histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(filtered_hashtag_df['Hashtag'], filtered_hashtag_df['Occurences'], color='skyblue')\n",
    "plt.xlabel('Hashtag')\n",
    "plt.ylabel('Occurences')\n",
    "plt.title('Hashtag Occurrences in the Typhoon Subgraph (Count >= 20)')\n",
    "plt.xticks(rotation=90) \n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hashtag \"RubyPH\" was by far the most used hashtag in the typhoon subgraph with 1,791 total occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bombing_hashtags_df = hashtag_dfs['bombing_subgraph']\n",
    "bombing_hashtags_df['Subgraph'] = \"Bombing\" \n",
    "\n",
    "filtered_hashtag_df = bombing_hashtags_df[bombing_hashtags_df['Occurences'] >= 20]\n",
    "\n",
    "# Plot the filtered DataFrame as a histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(filtered_hashtag_df['Hashtag'], filtered_hashtag_df['Occurences'], color='skyblue')\n",
    "plt.xlabel('Hashtag')\n",
    "plt.ylabel('Occurences')\n",
    "plt.title('Hashtag Occurrences in the Bombing Subgraph (Count >= 20)')\n",
    "plt.xticks(rotation=90)  \n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hashtag \"PrayForParis\" was by far the most used hashtag in the bombing subgraph with 325 total occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_hashtags_df = hashtag_dfs['flood_subgraph']\n",
    "flood_hashtags_df['Subgraph'] = \"Flood\"\n",
    "\n",
    "filtered_hashtag_df = flood_hashtags_df[flood_hashtags_df['Occurences'] >= 20]\n",
    "\n",
    "# Plot the filtered DataFrame as a histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(filtered_hashtag_df['Hashtag'], filtered_hashtag_df['Occurences'], color='skyblue')\n",
    "plt.xlabel('Hashtag')\n",
    "plt.ylabel('Occurences')\n",
    "plt.title('Hashtag Occurrences in the Flood Subgraph (Count >= 20)')\n",
    "plt.xticks(rotation=90) \n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hashtag \"yycflood\" was by far the most used hashtag in the flood subgraph with 533 total occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shooting_hashtags_df = hashtag_dfs['shooting_subgraph']\n",
    "shooting_hashtags_df['Subgraph'] = \"Shooting\"\n",
    "\n",
    "filtered_hashtag_df = shooting_hashtags_df[shooting_hashtags_df['Occurences'] >= 20]\n",
    "\n",
    "# Plot the filtered DataFrame as a histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(filtered_hashtag_df['Hashtag'], filtered_hashtag_df['Occurences'], color='skyblue')\n",
    "plt.xlabel('Hashtag')\n",
    "plt.ylabel('Occurences')\n",
    "plt.title('Hashtag Occurrences in the Shooting Subgraph (Count >= 20)')\n",
    "plt.xticks(rotation=90)  \n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hashtag \"Dallas\" was by far the most used hashtag in the shooting subgraph with 493 total occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_dfs = [wildfire_hashtags_df, earthquake_hashtags_df, typhoon_hashtags_df, bombing_hashtags_df, flood_hashtags_df, shooting_hashtags_df] \n",
    "\n",
    "# Concatenate the DataFrames along rows\n",
    "combined_hashtags_dfs = pd.concat(hashtags_dfs, ignore_index=True)\n",
    "\n",
    "combined_hashtags_dfs_sorted = combined_hashtags_dfs.sort_values(by='Occurences', ascending=False)\n",
    "\n",
    "# The top 25 rows\n",
    "top_hashtags = combined_hashtags_dfs_sorted.head(25)\n",
    "\n",
    "# Histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Hashtag', y='Occurences', hue='Subgraph', data=top_hashtags, palette=colors)\n",
    "plt.title('Top 25 Hashtags')\n",
    "plt.xlabel('Hashtag')\n",
    "plt.ylabel('Occurences')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 The Central Users Based on Different Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Twitter network, different centrality measures can provide insights into a user's ability to connect with others, spread information, and gather information. Here are centrality measures relevant to each of these aspects:\n",
    "\n",
    "**Connecting Users:**\n",
    "\n",
    "*Betweenness Centrality*: Users with high betweenness centrality act as bridges between different parts of the network, facilitating communication and connections between otherwise disconnected users or communities. They play a crucial role in connecting users and fostering network cohesion.\n",
    "\n",
    "Note that the calling of the beetweenness centrality function takes time, and that the code chunks can be long to run. Do not hesitate to comment the *nx.betweenness_centrality* function in the following function, that will highly speed up computation.\n",
    "\n",
    "**Spreading Information:**\n",
    "\n",
    "*Degree Centrality*: Users with high degree centrality are connected to a large number of other users through interactions such as mentions, retweets, replies, and follows. They have the potential to reach a wide audience and play a significant role in spreading information within the network.\n",
    "\n",
    "**Gathering Information:**\n",
    "\n",
    "*Closeness Centrality*: Users with high closeness centrality are close to other users in terms of shortest path distances, allowing them to quickly access information from a wide range of sources within the network. They are well-positioned to gather information efficiently and maintain awareness of network dynamics.\n",
    "\n",
    "\n",
    "In the following, we will only consider the user nodes of our network and calculate for each of our event-based subgraphs the top 5 users with the highest betweenness centrality, degree centrality and closeness centrality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us first define a function to compute the centralities measures of a given subgraph\n",
    "def centrality_function(graph):\n",
    "    # Filter nodes to only include user nodes\n",
    "    filtered_nodes = [node for node, data in graph.nodes(data=True) if ':User' in data.get('labels', '') or ':Tweet' in data.get('labels', '')]\n",
    "\n",
    "    # Create a subgraph containing only user nodes\n",
    "    user_subgraph = graph.subgraph(filtered_nodes)\n",
    "\n",
    "    # Calculate centralities for user nodes\n",
    "    centralities = {\n",
    "        'betweenness_centrality': nx.betweenness_centrality(user_subgraph),\n",
    "        'degree_centrality': nx.degree_centrality(user_subgraph),\n",
    "        'closeness_centrality': nx.closeness_centrality(user_subgraph)\n",
    "    }\n",
    "\n",
    "    # Identify top 3 users with the highest centralities per centrality measure\n",
    "    top_users_by_centrality = {}\n",
    "\n",
    "    for centrality_measure, centrality_dict in centralities.items():\n",
    "        top_users_by_centrality[centrality_measure] = sorted(centrality_dict.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "\n",
    "    # Print top 3 users with the highest centralities and all node attributes\n",
    "    for centrality_measure, top_users in top_users_by_centrality.items():\n",
    "        print(f\"Top 3 users by {centrality_measure}:\")\n",
    "        for user_id, centrality_score in top_users:\n",
    "            node_data = user_subgraph.nodes[user_id]\n",
    "            print(f\"\\tUser {user_id} - Centrality: {centrality_score}\")\n",
    "            for key, value in node_data.items():\n",
    "                print(f\"\\t\\t{key}: {value}\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Wildfire subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_function(wildfire_subgraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the wildfire subgraph:\n",
    "\n",
    "- User node n87002, who goes by the screen name \"yulsman\" and has 1,897 followers has the highest betweeness centrality. They are the best at connecting users.\n",
    "\n",
    "- User node n88567, who goes by the screen name \"BreakingNews\" and has 9,345,296 followers has the highest degree centrality. They are the best at spreading information. \n",
    "\n",
    "- User node n64463, who goes by the screen name \"AP\" (The Associated Press) and has 12,790,776 followers has the highest closeness centrality. They are the best at gathering information. \n",
    "\n",
    "For each centrality, the top 3 is listed above, with all their respective attributes, if one wants to gain more insight on other central users within our twitter network for each centrality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Earthquake Subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_function(earthquake_subgraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the earthquake subgraph:\n",
    "\n",
    "- User node n88567, who goes by the screen name \"BreakingNews\" and has 9,345,296 followers has the highest betweeness centrality. They are the best at connecting users.\n",
    "\n",
    "- User node n90039, who goes by the screen name \"NewEarthquake\" and has 891,454 followers has the highest degree centrality. They are the best at spreading information. \n",
    "\n",
    "- User node n96126, who goes by the screen name \"narendramodi\" (the prime minister of India) and has 43,346,394 followers has the highest closeness centrality. They are the best at gathering information. \n",
    "\n",
    "For each centrality, the top 3 is listed above, with all their respective attributes, if one wants to gain more insight on other central users within our twitter network for each centrality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Shooting Subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_function(shooting_subgraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the shooting subgraph:\n",
    "\n",
    "- User node n83848, who goes by the screen name \"CNN\" and has 41,613,138 followers has the highest betweeness centrality. They are the best at connecting users.\n",
    "\n",
    "- User node n96724, who goes by the screen name \"realDonaldTrump\" (the previous Uninted States president) and has 48,268,973 followers has the highest degree centrality. They are the best at spreading information. \n",
    "\n",
    "- User node n96724 (again), who goes by the screen name \"realDonaldTrump\" and has 48,268,973 followers has the highest closeness centrality. They are the best at gathering information. \n",
    "\n",
    "For each centrality, the top 3 is listed above, with all their respective attributes, if one wants to gain more insight on other central users within our twitter network for each centrality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Bombing Subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_function(bombing_subgraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the bombing subgraph:\n",
    "\n",
    "- User node n88246, who goes by the screen name \"cnnbrk\" (CNN Breaking News) and has 55,261,111 followers has the highest betweeness centrality. They are the best at connecting users.\n",
    "\n",
    "- User node n64463, who goes by the screen name \"AP\" (The Associated Press) and has 12,790,776 followers has the highest degree centrality. They are the best at spreading information. \n",
    "\n",
    "- User node n64463 (again), who goes by the screen name \"AP\" (The Associated Press) and has 12,790,776 followers has the highest closeness centrality. They are the best at gathering information. \n",
    "\n",
    "For each centrality, the top 3 is listed above, with all their respective attributes, if one wants to gain more insight on other central users within our twitter network for each centrality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 Flood Subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_function(flood_subgraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the flood subgraph:\n",
    "\n",
    "- User node n98043, who goes by the screen name \"nenshi\" and has 376,529 followers has the highest betweeness centrality. They are the best at connecting users.\n",
    "\n",
    "- User node n58501, who goes by the screen name \"ANCALERTS\" (ABS-CBN News Channel) and has 4,426,900 followers has the highest degree centrality. They are the best at spreading information. \n",
    "\n",
    "- User node n94255, who goes by the screen name \"CyrilRamaphosa\" (Cyril Ramaphosa, the president of South Africa) and has 569,816 followers has the highest closeness centrality. They are the best at gathering information. \n",
    "\n",
    "For each centrality, the top 3 is listed above, with all their respective attributes, if one wants to gain more insight on other central users within our twitter network for each centrality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.6 Typhoon Subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_function(typhoon_subgraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the typhoon subgraph:\n",
    "\n",
    "- User node n58501, who goes by the screen name \"ANCALERTS\" (ABS-CBN News Channel) and has 4,426,900 followers has the highest betweeness centrality. They are the best at connecting users.\n",
    "\n",
    "- User node n58501 (again), who goes by the screen name \"ANCALERTS\" (ABS-CBN News Channel) and has 4,426,900 followers has the highest degree centrality. They are the best at spreading information. \n",
    "\n",
    "- User node n58317, who goes by the screen name \"dost_pagasa\" and has 5,619,485 followers has the highest closeness centrality. They are the best at gathering information. \n",
    "\n",
    "For each centrality, the top 3 is listed above, with all their respective attributes, if one wants to gain more insight on other central users within our twitter network for each centrality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice, for all subgraphs, that news outlets and the press such as CNN, ABC news, The Asscociated Press, often have the highest betweeness and/or degree centrality. They are the best at connecting users and spreading information. Given their nature, this make a lot of sence. Furthermore, we notice that influencers and polititians often have the highest closeness centrality and are the best at gathering information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 The Distributions of the Tweet Posts and their Main Topics w.r.t Levels of Criticality Among the “Low”, “Medium”, “High” and “Critical”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will here analyze the different kinds of events in the twitter network related to different crises. \n",
    "\n",
    "There are 6 different types of events :\n",
    "\n",
    "- *Bombings*\n",
    "- *Earthquakes*\n",
    "- *Floods*\n",
    "- *Shootings*\n",
    "- *Typhoons*\n",
    "- *Wildfires*\n",
    "\n",
    "(Note: To view our analysis for each subgraph, one must change the variable 'chosen_event' below, and run the subsequent code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_event = 'bombing' #can modify this variable to 'wildfire', 'earthquake', 'flood', 'shooting', 'typhoon' or 'bombing'\n",
    "g = subgraphs.get(chosen_event + '_subgraph')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us extract all the tweets from the 'chosen_event' events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "for node, data in g.nodes(data=True):\n",
    "    for key, value in data.items():\n",
    "        if key == 'labels':\n",
    "            if value == ':Tweet':\n",
    "                tweets.append({node : data})\n",
    "\n",
    "print(f\"There are {len(tweets)} tweets for the {chosen_event} event type\\n\")\n",
    "print(\"Example of tweet :\")\n",
    "print(tweets[0])\n",
    "\n",
    "# we remove unknown criticality tweets\n",
    "tweets = [tweet for tweet in tweets if tweet[list(tweet.keys())[0]]['annotation_postPriority'] != 'Unknown']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we extract the events names and add them to the tweets dictionnary for better visualisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = []\n",
    "for node, data in g.nodes(data=True):\n",
    "    for key, value in data.items():\n",
    "        if key == 'labels':\n",
    "            if value == ':Event':\n",
    "                events.append({node : data})\n",
    "\n",
    "events = [list(dict(events[i].items()).values())[0] for i in range(len(events))]\n",
    "events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tweets)) : \n",
    "    event_id = list(dict(tweets[i].items()).values())[0]['topic']\n",
    "    for item in events:\n",
    "        if item.get('trecisid') == event_id:\n",
    "            desired_event = item\n",
    "\n",
    "    for inner_dict in tweets[i].values():\n",
    "        inner_dict['ref_event'] = desired_event['id']\n",
    "\n",
    "# we have the reference event at the end\n",
    "tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for each of the tweets, we have added an attribute \"ref_event\" to their attribute dictionnary. This represents the name of the referenced event in the tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.1 Criticality of the tweets count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we convert the list of dictionnaries into a pandas dataframe which will be easier to manipulate\n",
    "list_of_tweets = [list(dict(tweets[i].items()).values())[0] for i in range(len(tweets))]\n",
    "df_tweets = pd.DataFrame(list_of_tweets)\n",
    "df_tweets.head()\n",
    "\n",
    "# visual purposes\n",
    "order_crit = ['Critical', 'High', 'Medium', 'Low']\n",
    "palette = sns.color_palette(\"coolwarm\")\n",
    "palette = palette[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the distribution of the criticality of the tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['annotation_postPriority'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "sns.countplot(y=df_tweets['annotation_postPriority'], color='lightblue', order=order_crit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice, for all event-types, that:\n",
    "- low criticality tweets are more common than medium centrality tweets\n",
    "- medium criticality tweets are more common than high centrality tweets\n",
    "- high criticality tweets are more common than critical centrality tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Tweet count for each sub-event, separated by level of criticality\n",
    "(Note: we define a sub-event as a smaller event of a bigger general event type, for example the colorado fire is a sub-event of event 'wildfire')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(data=df_tweets, \n",
    "                   x='ref_event', \n",
    "                   hue='annotation_postPriority', \n",
    "                   palette=palette, \n",
    "                   hue_order=order_crit)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For all event types:\n",
    "\n",
    "We observe that the majority of tweets are of low priority generally speaking. They usually represent the majority of tweets within sub-events too. However, we still notice some high and critical tweets, which will be interesting to study. An exception arrises when studying the flood and shooting event types, where for the South Africa flood of 2019 and San Diego shooting of 2019, we have high and medium priority tweets respectively that are far more common. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Social activity associated with the tweets by level of criticality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 5))\n",
    "sns.violinplot(data = df_tweets,\n",
    "               x = 'annotation_postPriority',\n",
    "               y = 'retweet_count',\n",
    "               order = order_crit,\n",
    "               color = 'lightblue')\n",
    "plt.ylim(0, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For wildfire event type: \n",
    "We observe that while there are numerous outliers with very high retweet counts, likely attributed to tweets from influential or famous users, the overall trend indicates that as tweet criticality increases, tweets tend to receive more retweets on average. This suggests active engagement from the Twitter community in amplifying important information.\n",
    "\n",
    "- For earthquake event type:\n",
    "We observe that while there are numerous outliers with high, medium and low retweet counts, likely attributed to tweets from influential or famous users, we observe that critical tweets receive far more retweets on average compared to other criticality types. This is followed by medium and low tweets and finally high criticality tweets who appear to receive far less retweets on average. This suggests active engagement from the Twitter community in amplifying critical information more than anything else.\n",
    "\n",
    "- For flood and shooting event types:\n",
    "We observe numerous outliers with high, medium and low retweet counts. Little outliers for critical priority tweets. Contrary to the two previous event types analysed, the critical, high and medium priority tweets appear to have far less average retweets than low priority tweets. This suggests active engagement from the Twitter community in amplifying low priority information more than anything else.\n",
    "\n",
    "- For typhoon event type: \n",
    "We observe many outliers for all the tweet prioprity types. Medium criticality tweets seem to be far more retweeted on average than all the rest. The average retweets seem to be comparable for the other criticality types. This suggests active engagement from the Twitter community in amplifying medium priority information more than anything else.\n",
    "\n",
    "- For bombing event type: \n",
    "We observe little outliers all around. There are far more average retweet counts than the rest of the event types. High criticality tweets seem to be the tweets with the most average retweets by far, followed by medium criticality tweets. This suggests active engagement from the Twitter community in amplifying medium priority information more than anything else.\n",
    "\n",
    "This can be confirmed by looking at the mean number of retweets by tweet priority :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also look at the mean number of retweets for each level of criticality\n",
    "df_tweets[['annotation_postPriority', 'retweet_count']].groupby('annotation_postPriority').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4 Which events contain the more critical tweets and which are the most retweeted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.boxplot(data=df_tweets[df_tweets['retweet_count'] < 500], \n",
    "                 x='ref_event', \n",
    "                 y='retweet_count', \n",
    "                 hue='annotation_postPriority', \n",
    "                 showfliers=False,\n",
    "                 palette=palette,\n",
    "                 hue_order=order_crit)\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For wildfire event-type:\n",
    "The result found in 1.3.3 is confirmed here when we separate the tweets by sub-event. We clearly see the higher values are associated with the higher priority level of tweets in most cases.\n",
    "\n",
    "- For earthquake, flood, shooting and typhoon event-types:\n",
    "The result found in 1.3.3 is not necessarily confirmed here when we separate the tweets by sub-event. We notice that the prevelence of retweets by post priority changes from one sub-event to the next.\n",
    "\n",
    "- For bombing event-types:\n",
    "We notice that the prevelence of retweets by post priority does not change from one sub-event to the next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.5 What are the 'favourite counts' for each level of criticality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 5))\n",
    "sns.violinplot(data = df_tweets,\n",
    "               x = 'annotation_postPriority',\n",
    "               y = 'favorite_count',\n",
    "               order = order_crit,\n",
    "               color = 'lightblue')\n",
    "plt.ylim(0, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For wildfire, earthquake, flood and shooting event types:\n",
    "The trend seems to be the same than for the retweet counts as the violin plots for each tweet criticality level look almost similar. \n",
    "\n",
    "- For typhoon event type:\n",
    "Favorite counts seem to be more important for critical priority tweets, rather than medium priority tweets which had the highest average retweets.\n",
    "\n",
    "- For bombing event-types:\n",
    "Favorite counts seem to be more important for low priority tweets, rather than medium priority tweets which had the highest average retweets.\n",
    "\n",
    "\n",
    "Let us confirm this intuition by looking at the mean number of favorite for each level of criticality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets[['annotation_postPriority', 'favorite_count']].groupby('annotation_postPriority').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For wildfire event type:\n",
    "The same ranking as before can be observed. Critical tweets has a substancially higher favorite count than high tweets, followed by medium and then low tweets.\n",
    "\n",
    "- For earthquake event type:\n",
    "We observe that while there are numerous outliers with high, medium and low favorite counts, we observe that critical tweets receive far more favorites on average compared to other criticality types. This is followed by medium and low tweets and finally high criticality tweets where high tweets appear to receive far less favrites on average. This suggests that critical priority tweets are far more loved by the Twitter community in than anything else.\n",
    "\n",
    "- For flood event type:\n",
    "We observe that there are many outliers with high, medium and low favorite counts. There are also very little favorites on average for critical criticality tweets. The type of tweet with the most favorites on average is by far the medium priority tweets.\n",
    "\n",
    "- For flood event type:\n",
    "We observe that there are many outliers with medium and low favorite counts. There are also very little favorites on average for critical criticality tweets. The type of tweet with the most favorites on average is by far the low priority tweets.\n",
    "\n",
    "- For typhoon event type:\n",
    "Favorite counts are greater for critical priority tweets, than any other types of tweets. Many outliers are present for all priority types.\n",
    "\n",
    "- For bombing event-types:\n",
    "Favorite counts seem to be far more important for low priority tweets. Many outliers are observed for these types of tweets too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.6 Sentitivity of the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 5))\n",
    "sns.countplot(data = df_tweets,\n",
    "              x = 'annotation_postPriority',\n",
    "              hue = 'possibly_sensitive',\n",
    "              hue_order=[True, False],\n",
    "              palette = [palette[0], palette[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For wildfire event type:\n",
    "We see here that the relative distribution of possibly sensitive tweets is quite comparable between different levels of criticality. It is important to note that usually possiblibly sensitive tweets are fewer in numbers than 'normal' tweets. However, this is not the case for the low criticality tweets.\n",
    "\n",
    "- For earthquake event type:\n",
    "We see here that the relative distribution of possibly sensitive tweets is quite comparable between different levels of criticality. It is important to note that usually possiblibly sensitive tweets are fewer in numbers than 'normal' tweets for all priority types. The gap is smaller for low priority tweets.\n",
    "\n",
    "- For flood and shooting event type:\n",
    "We see here that the relative distribution of possibly sensitive tweets is quite comparable between different levels of criticality. It is important to note that usually possiblibly sensitive tweets are fewer in numbers, it is not the case for low priority tweets that have a higher frequency of possibly sensitive tweets here.\n",
    "\n",
    "- For typhoon event type:\n",
    "Possibly non sensitive tweets are far better represented for all the criticality tweets types.\n",
    "\n",
    "- For bombing event-types:\n",
    "Possibly non sensitive tweets are better represented for all the criticality tweets types, except for critical priority tweets where posibly sensitive tweets slightly edge non sensitive ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.7 Comparing the contents of tweets having different levels of criticality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = df_tweets[df_tweets.annotation_postPriority == 'Critical'].reset_index()['text'][0]\n",
    "high = df_tweets[df_tweets.annotation_postPriority == 'High'].reset_index()['text'][0]\n",
    "medium = df_tweets[df_tweets.annotation_postPriority == 'Medium'].reset_index()['text'][0]\n",
    "low = df_tweets[df_tweets.annotation_postPriority == 'Low'].reset_index()['text'][0]\n",
    "print(crit)\n",
    "print()\n",
    "print(high)\n",
    "print()\n",
    "print(medium)\n",
    "print()\n",
    "print(low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For wildfire event type:\n",
    "This example presents interesting distinctions. The critical tweet provides vital information about ongoing events, such as potential solutions to navigate a closed road. The high criticality tweet provides slightly less vital information but it stays relatively informaive. The medium critically tweet is more descriptive than the two previous ones, however there still is some valuable information about the direction of the fire.\n",
    "\n",
    "Conversely, the low-priority tweet expresses gratitude after the event, directed towards those who participated, which is in stark contrast to the previously described tweets.\n",
    "\n",
    "This contrast highlights a pattern: high-priority tweets often offer actionable advice to mitigate risks during events, whereas low-priority tweets tend to offer general reflections on the event without specific guidance or recommendations.\n",
    "\n",
    "- For earthquake event type:\n",
    "This example presents interesting distinctions. The critical tweet provides good general tips. The high priority tweet provides descriptive updates on the situation. The medium priority tweet provides descriptive information presented in an informal manner. THe low priority tweet provides personal information.\n",
    "\n",
    "- For flood event type:\n",
    "The critical and high priority tweets provides descriptive information about infrastructure damage and death toll respectively. THe medium priority tweet shows political developments. Finally, the low priority tweet gives reasurence that rice can survive underwater.\n",
    "\n",
    "- For shooting event type:\n",
    "The critical and high priority tweets provides descriptive information about ongoing developments, about the shooter's capture and casualty toll respectively. THe medium priority tweet gives information about both the casulties and the shooters capture. Finally, the low priority tweet gives  a minor descriptive event.\n",
    "\n",
    "- For typhoon event type:\n",
    "The critical and high priority tweets provide news updates and information about ongoing developments. The medium criticality tweet gives us a fact about the magnitude of the typhoon. Finally the low criticality tweet is giberish.\n",
    "\n",
    "- For bombing event type:\n",
    "The critical and high priority tweets provide breaking news updates and information about ongoing developments. The medium criticality tweet gives us descriptions of the ongoing event. Finally the low criticality tweet is more personal and emotional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embeddings and User-User similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required packages :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "%pip install node2vec\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "%pip install gensim\n",
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the event to be analyzed, and import the graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_event = 'bombing' #can modify this variable to 'wildfire', 'earthquake', 'flood', 'shooting', 'typhoon' or 'bombing'\n",
    "g = subgraphs.get(chosen_event + '_subgraph')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Sampling n users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to sample n users based on the centralities of the users in the chosen subgraph. \n",
    "\n",
    "- We will then take :\n",
    "    - The 50 more central users based on their degree centrality \n",
    "    - The 50 more central users based on their closeness centrality  \n",
    "\n",
    "- This means we will get between 50 and 100 users sampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = []\n",
    "nodes_users = []\n",
    "for node, data in g.nodes(data=True):\n",
    "    for key, value in data.items():\n",
    "        if key == 'labels':\n",
    "            if value == ':User':\n",
    "                nodes_users.append(node)\n",
    "                users.append({node : data})\n",
    "\n",
    "#We choose the size of the sample\n",
    "n=100\n",
    "\n",
    "# Sample n/2 users with highest degree centrality\n",
    "degree_centralities = nx.degree_centrality(g) #We first compute the degree coef fo all nodes\n",
    "degree_filtred = {key: value for key, value in degree_centralities.items() if key in nodes_users} #We then only select the users nodes\n",
    "top_degree_users = sorted(degree_filtred, key=lambda x: degree_filtred[x], reverse=True)[0:int(n/2)] #We take the 50 users that have the highest degree centrality\n",
    "\n",
    "\n",
    "filtered_nodes = [node for node in nodes_users if node not in top_degree_users] #We create a list of all the users except the ones already in the top 50 of degree centrality\n",
    "\n",
    "# Sample n/2 users with highest closeness centrality (taking away the nodes that are already in the highest degree)\n",
    "closeness_centralities = nx.closeness_centrality(g) #We first compute the closeness coef fo all nodes\n",
    "closeness_filtred = {key: value for key, value in closeness_centralities.items() if key in filtered_nodes} #We then only select the users nodes (that are not in the top 50 degree centrality)\n",
    "top_closeness_users = sorted(closeness_filtred, key=lambda x: closeness_filtred[x], reverse=True)[0:int(n/2)]  #We take the 50 users that have the highest closeness centrality\n",
    "\n",
    "# List of the selected users\n",
    "list_user = list(set(top_degree_users + top_closeness_users))\n",
    "\n",
    "users_sample = []\n",
    "\n",
    "for node_dict in users:\n",
    "    node_key = list(node_dict.keys())[0]  \n",
    "    if node_key in list_user:  \n",
    "        users_sample.append({node_key: node_dict[node_key]})\n",
    "\n",
    "# exemple of user \n",
    "users_sample[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.ii.a - Embeddings on Graph Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the subgraph containing our sampled users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we need the associated nodes ids to the sample we definded : \n",
    "node_sample_ids = []\n",
    "for i in range(len(users_sample)):\n",
    "    for key, _ in users_sample[i].items():\n",
    "        node_sample_ids.append(key)\n",
    "\n",
    "# define the subgraph with the sample of 100 nodes\n",
    "sample_graph = g.subgraph(node_sample_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to fit a Node2Vec model on our sub-subgraph, that will create embeddings from the graph structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 100/100 [00:00<00:00, 19378.60it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 10/10 [00:00<00:00, 245.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# we can then build the embeddings of the sampled graph with Node2Vec\n",
    "node2vec = Node2Vec(sample_graph, dimensions=50)\n",
    "fitted_model_n2v = node2vec.fit(window=10, min_count=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have access to the 10 most similar nodes of a node, based on cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in users_sample[0].items():\n",
    "    k = key\n",
    "fitted_model_n2v.wv.most_similar(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can build a cosine similarity matrix :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the embeddings for every node\n",
    "node_embeddings_n2v = {node: fitted_model_n2v.wv[node] for node in sample_graph.nodes()}\n",
    "\n",
    "# list element is easier to handle\n",
    "list_of_embeddings_n2v = []\n",
    "for key, value in node_embeddings_n2v.items():\n",
    "    list_of_embeddings_n2v.append({key : value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the cosine similarity matrix\n",
    "cos_sim_n2v = [[0 for _ in range(100)] for _ in range(100)]\n",
    "\n",
    "for i, emb_i_dict in enumerate(list_of_embeddings_n2v):\n",
    "    for j, emb_j_dict in enumerate(list_of_embeddings_n2v):\n",
    "        emb_i = next(iter(emb_i_dict.values())) \n",
    "        emb_j = next(iter(emb_j_dict.values()))\n",
    "\n",
    "        cosine_sim = cosine_similarity([emb_i], [emb_j])[0][0]\n",
    "        cos_sim_n2v[i][j] = cosine_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this cosine similarity matrix, we can easily get a list of highly similar users :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from this, we can get the more similar users in our sample graph :\n",
    "arr_cos_sim_n2v = np.array(cos_sim_n2v)\n",
    "np.fill_diagonal(arr_cos_sim_n2v, -np.inf) # we replace the 1 elements of the diagonal by -inf\n",
    "\n",
    "# 10 most similar nodes\n",
    "v = []\n",
    "\n",
    "nb_users_to_print = 10\n",
    "for _ in range(nb_users_to_print):\n",
    "    max_index = np.argmax(arr_cos_sim_n2v)\n",
    "    max_sim = np.max(arr_cos_sim_n2v)\n",
    "\n",
    "    max_row_index, max_col_index = np.unravel_index(max_index, np.array(arr_cos_sim_n2v).shape)\n",
    "\n",
    "    arr_cos_sim_n2v[max_row_index][max_col_index] = -np.inf\n",
    "    arr_cos_sim_n2v[max_col_index][max_row_index] = -np.inf\n",
    "\n",
    "    v.append((max_row_index, max_col_index, max_sim))\n",
    "\n",
    "print(f\"The {nb_users_to_print} most similar pairs of users are :\")\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.ii.b - Embeddings on Post Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a list of tweets containing the tweets that were posted by the users in our previously created subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we already have our sampled graph and the users, we now need the tweets\n",
    "tweets = []\n",
    "for node, data in g.nodes(data=True):\n",
    "    for key, value in data.items():\n",
    "        if key == 'labels':\n",
    "            if value == ':Tweet':\n",
    "                tweets.append({node : data})\n",
    "\n",
    "# exemple of tweet\n",
    "tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need a list of dictionnaries containing the user_id as keys, and their posted tweets as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a list containing dictionnaries with the user_node_id and every tweet made by this user\n",
    "users_posts = []\n",
    "\n",
    "for u in users_sample:\n",
    "    user_node_id = [key for key, _ in u.items()][0]\n",
    "    tweets_by_user = []\n",
    "    \n",
    "    for t in tweets:\n",
    "        tweet_node_id = [key for key, _ in t.items()][0]\n",
    "        text_tweet = [value for _, value in t.items()][0]['text']\n",
    "\n",
    "        if tweet_node_id in g[user_node_id]:\n",
    "            tweets_by_user.append(text_tweet)\n",
    "\n",
    "    users_posts.append({'user':user_node_id,\n",
    "                        'tweets':tweets_by_user})\n",
    "    \n",
    "# exemple of users posts :\n",
    "users_posts[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load a pretrained Word2Vec model on tweeter data, that will convert the posts by users into embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us build the empbeddings of our tweets, using a pretrained Word2Vec model on twitter data, using embeddings of length 50 to match the graph structure embeddings\n",
    "w2v = gensim.downloader.load('glove-twitter-50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a function that will create the embedding of a given tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define a function that allows us to tokenize a tweet/sentence, and we take the average of each embedding\n",
    "def sentence_embedding(sentence):\n",
    "    if len(sentence)>0:\n",
    "        tokens = sentence.split()\n",
    "        embeddings = [w2v[token] for token in tokens if token in w2v]\n",
    "        \n",
    "        if len(tokens) > 0 and embeddings:\n",
    "                avg_embedding = np.mean(embeddings, axis=0)\n",
    "        else:\n",
    "            avg_embedding = np.zeros(w2v.vector_size)\n",
    "    else:\n",
    "         avg_embedding = np.zeros(w2v.vector_size)\n",
    "    return avg_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compute the embeddings of our set of tweets and add them to the list of dictionnaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we then compute and add the embedded tweets to the users_posts list, by adding a key to each dictionnary in the list\n",
    "for i in users_posts:\n",
    "    tweets = i['tweets']\n",
    "    embedded_tweets = []\n",
    "\n",
    "    if len(tweets) == 0:\n",
    "        avg_emb_tweet = np.zeros(w2v.vector_size)\n",
    "\n",
    "    else :\n",
    "        for j in i['tweets']:\n",
    "            emb_tweet = sentence_embedding(j)\n",
    "            embedded_tweets.append(emb_tweet)\n",
    "            avg_emb_tweet = np.mean(embedded_tweets, axis=0)\n",
    "    i['embedded_tweets'] = avg_emb_tweet\n",
    "\n",
    "# exemple of new user post :\n",
    "users_posts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can create a cosine similarity matrxi based on our embedded tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now compute the cosine similarity matrix like with our graph structure embeddings\n",
    "cos_sim_w2v = [[0 for _ in range(100)] for _ in range(100)]\n",
    "\n",
    "for i in range(len(users_posts)):\n",
    "    for j in range(len(users_posts)):\n",
    "        if i == j:\n",
    "            cos_sim_w2v[i][j] = 1\n",
    "        else:\n",
    "            emb_i = users_posts[i]['embedded_tweets'].reshape(1, -1)\n",
    "            emb_j = users_posts[j]['embedded_tweets'].reshape(1, -1)\n",
    "            cos_sim_w2v[i][j] = cosine_similarity(emb_i, emb_j)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can get a list of very similar users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, we can print the most similar users based on this matrix\n",
    "arr_cos_sim_w2v = np.array(cos_sim_w2v)\n",
    "np.fill_diagonal(arr_cos_sim_w2v, -np.inf) # replace the diagonal full of 1s by -inf \n",
    "\n",
    "v = []\n",
    "\n",
    "nb_users_to_print = 10\n",
    "for _ in range(nb_users_to_print):\n",
    "    max_index = np.argmax(arr_cos_sim_w2v)\n",
    "    max_sim = np.max(arr_cos_sim_w2v)\n",
    "\n",
    "    max_row_index, max_col_index = np.unravel_index(max_index, np.array(arr_cos_sim_w2v).shape)\n",
    "\n",
    "    arr_cos_sim_w2v[max_row_index][max_col_index] = -np.inf\n",
    "    arr_cos_sim_w2v[max_col_index][max_row_index] = -np.inf\n",
    "    \n",
    "    v.append((max_row_index, max_col_index, max_sim))\n",
    "\n",
    "print(f\"The {nb_users_to_print} more similar pairs of users are :\")\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemple of tweets, we can compare them \n",
    "print(users_posts[v[1][0]]['tweets'])\n",
    "print()\n",
    "print(users_posts[v[1][1]]['tweets'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the earthquake subgraph for example, it seems that both most similar tweets are talking mainly about events that happened in Nepal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.iii - Trends in Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a general function that summarizes all the steps we just realized, to be looped for all event types\n",
    "\n",
    "- This function will use the Word2Vec pretrained model that we loaded before, so make sure you have loaded it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_matrix(selected_graph):\n",
    "\n",
    "    g = subgraphs.get(selected_graph + '_subgraph')\n",
    "\n",
    "    users = []\n",
    "    nodes_users = []\n",
    "    for node, data in g.nodes(data=True):\n",
    "        for key, value in data.items():\n",
    "            if key == 'labels':\n",
    "                if value == ':User':\n",
    "                    nodes_users.append(node)\n",
    "                    users.append({node : data})\n",
    "\n",
    "    #We choose the size of the sample\n",
    "    n=100\n",
    "\n",
    "    # Sample n/2 users with highest degree centrality\n",
    "    degree_centralities = nx.degree_centrality(g) #We first compute the degree coef fo all nodes\n",
    "    degree_filtred = {key: value for key, value in degree_centralities.items() if key in nodes_users} #We then only select the users nodes\n",
    "    top_degree_users = sorted(degree_filtred, key=lambda x: degree_filtred[x], reverse=True)[0:int(n/2)] #We take the 50 users that have the highest degree centrality\n",
    "\n",
    "\n",
    "    filtered_nodes = [node for node in nodes_users if node not in top_degree_users] #We create a list of all the users except the ones already in the top 50 of degree centrality\n",
    "\n",
    "    # Sample n/2 users with highest closeness centrality (taking away the nodes that are already in the highest degree)\n",
    "    closeness_centralities = nx.closeness_centrality(g) #We first compute the closeness coef fo all nodes\n",
    "    closeness_filtred = {key: value for key, value in closeness_centralities.items() if key in filtered_nodes} #We then only select the users nodes (that are not in the top 50 degree centrality)\n",
    "    top_closeness_users = sorted(closeness_filtred, key=lambda x: closeness_filtred[x], reverse=True)[0:int(n/2)]  #We take the 50 users that have the highest closeness centrality\n",
    "\n",
    "    # List of the selected users\n",
    "    list_user = list(set(top_degree_users + top_closeness_users))\n",
    "\n",
    "    users_sample = []\n",
    "\n",
    "    for node_dict in users:\n",
    "        node_key = list(node_dict.keys())[0]  \n",
    "        if node_key in list_user:  \n",
    "            users_sample.append({node_key: node_dict[node_key]})\n",
    "\n",
    "\n",
    "    # first, we need the associated nodes ids to the sample we definded : \n",
    "    node_sample_ids = []\n",
    "    for i in range(len(users_sample)):\n",
    "        for key, _ in users_sample[i].items():\n",
    "            node_sample_ids.append(key)\n",
    "\n",
    "    # define the subgraph with the sample of 100 nodes\n",
    "    sample_graph = g.subgraph(node_sample_ids)\n",
    "\n",
    "    # we can then build the embeddings of the sampled graph with Node2Vec\n",
    "    node2vec = Node2Vec(sample_graph, dimensions=50)\n",
    "    fitted_model_n2v = node2vec.fit(window=10, min_count=1) \n",
    "    \n",
    "    # get the embeddings for every node\n",
    "    node_embeddings_n2v = {node: fitted_model_n2v.wv[node] for node in sample_graph.nodes()}\n",
    "\n",
    "    # list element is easier to handle\n",
    "    list_of_embeddings_n2v = []\n",
    "    for key, value in node_embeddings_n2v.items():\n",
    "        list_of_embeddings_n2v.append({key : value})\n",
    "\n",
    "    # build the cosine similarity matrix\n",
    "    cos_sim_n2v = [[0 for _ in range(100)] for _ in range(100)]\n",
    "\n",
    "    for i, emb_i_dict in enumerate(list_of_embeddings_n2v):\n",
    "        for j, emb_j_dict in enumerate(list_of_embeddings_n2v):\n",
    "            emb_i = next(iter(emb_i_dict.values())) \n",
    "            emb_j = next(iter(emb_j_dict.values()))\n",
    "\n",
    "            cosine_sim = cosine_similarity([emb_i], [emb_j])[0][0]\n",
    "            cos_sim_n2v[i][j] = cosine_sim\n",
    "\n",
    "\n",
    "    # we already have our sampled graph and the users, we now need the tweets\n",
    "    tweets = []\n",
    "    for node, data in g.nodes(data=True):\n",
    "        for key, value in data.items():\n",
    "            if key == 'labels':\n",
    "                if value == ':Tweet':\n",
    "                    tweets.append({node : data})\n",
    "\n",
    "\n",
    "    # build a list containing dictionnaries with the user_node_id and every tweet made by this user\n",
    "    users_posts = []\n",
    "\n",
    "    for u in users_sample:\n",
    "        user_node_id = [key for key, _ in u.items()][0]\n",
    "        tweets_by_user = []\n",
    "        \n",
    "        for t in tweets:\n",
    "            tweet_node_id = [key for key, _ in t.items()][0]\n",
    "            text_tweet = [value for _, value in t.items()][0]['text']\n",
    "\n",
    "            if tweet_node_id in g[user_node_id]:\n",
    "                tweets_by_user.append(text_tweet)\n",
    "\n",
    "        users_posts.append({'user':user_node_id,\n",
    "                            'tweets':tweets_by_user})    \n",
    "\n",
    "\n",
    "    # we then compute and add the embedded tweets to the users_posts list, by adding a key to each dictionnary in the list\n",
    "    for i in users_posts:\n",
    "        tweets = i['tweets']\n",
    "        embedded_tweets = []\n",
    "\n",
    "        if len(tweets) == 0:\n",
    "            avg_emb_tweet = np.zeros(w2v.vector_size)\n",
    "\n",
    "        else :\n",
    "            for j in i['tweets']:\n",
    "                emb_tweet = sentence_embedding(j)\n",
    "                embedded_tweets.append(emb_tweet)\n",
    "                avg_emb_tweet = np.mean(embedded_tweets, axis=0)\n",
    "        i['embedded_tweets'] = avg_emb_tweet\n",
    "\n",
    "    # we can now compute the cosine similarity matrix like with our graph structure embeddings\n",
    "    cos_sim_w2v = [[0 for _ in range(100)] for _ in range(100)]\n",
    "\n",
    "    for i in range(len(users_posts)):\n",
    "        for j in range(len(users_posts)):\n",
    "            if i == j:\n",
    "                cos_sim_w2v[i][j] = 1\n",
    "            else:\n",
    "                emb_i = users_posts[i]['embedded_tweets'].reshape(1, -1)\n",
    "                emb_j = users_posts[j]['embedded_tweets'].reshape(1, -1)\n",
    "                cos_sim_w2v[i][j] = cosine_similarity(emb_i, emb_j)[0][0]\n",
    "    return([cos_sim_n2v, cos_sim_w2v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop the function through all the subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_results = []\n",
    "\n",
    "events = ['wildfire', 'earthquake', 'typhoon', 'bombing', 'flood', 'shooting']\n",
    "\n",
    "for event_type in events:\n",
    "\n",
    "    res = similarity_matrix(event_type)\n",
    "    \n",
    "    graph_similarities = [similarity for sublist in res[0] for similarity in sublist]\n",
    "    content_similarities = [similarity for sublist in res[1] for similarity in sublist]\n",
    "\n",
    "    # Compute the correlation between the graph structure and tweet content similarities\n",
    "    correlation, p_value = spearmanr(graph_similarities, content_similarities)\n",
    "    \n",
    "    # Append the correlation coefficient and p-value to the results list\n",
    "    correlation_results.append({'event_type': event_type, 'correlation': correlation, 'p_value': p_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this results, we see that the similarity matrices for the graph structure and post contents are more correlated for the flood and shooting event (higher p-value) than for the earthquake and typhoon events. Moreover we see that in general we observe a positive correlation between the two matrices of similarity except for the shooting event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Information Retrieval System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import the nltk package that we will need for stopwords management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us re-import the graph to be sure we have the correct file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'database_formated_for_NetworkX.graphml'\n",
    "graph = nx.read_graphml(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need the users and tweets of our graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the users of our graph\n",
    "users = []\n",
    "nodes_users = []\n",
    "for node, data in graph.nodes(data=True):\n",
    "    for key, value in data.items():\n",
    "        if key == 'labels':\n",
    "            if value == ':User':\n",
    "                nodes_users.append(node)\n",
    "                users.append({node : data})\n",
    "\n",
    "# exemple of user\n",
    "print(users[0])\n",
    "\n",
    "# retrieve the tweets of our graph\n",
    "tweets = []\n",
    "for node, data in graph.nodes(data=True):\n",
    "    for key, value in data.items():\n",
    "        if key == 'labels':\n",
    "            if value == ':Tweet':\n",
    "                tweets.append({node : data['text']})\n",
    "\n",
    "# exemple of tweet\n",
    "print(tweets[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us build a list containing the Poster of a tweet, his tweet, and the degree centrality of the poster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_and_poster = []\n",
    "list_centralities = nx.degree_centrality(graph)\n",
    "\n",
    "for t in tweets:\n",
    "    for key, value in t.items():\n",
    "        tweet_id = key\n",
    "    neighb = list(graph.neighbors(tweet_id))\n",
    "    for n in neighb:\n",
    "        if graph.nodes[n]['labels'] == ':User':\n",
    "            tweets_and_poster.append({'poster' : n,\n",
    "                                      'tweet' : value,\n",
    "                                      'deg_centrality' : list_centralities[n]})\n",
    "\n",
    "# exmple \n",
    "tweets_and_poster[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the stopwords from all of our tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import english stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# function to remove the stopwords from a tweet\n",
    "def remove_stopwords(tweet):\n",
    "    words = tweet.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords from all the tweets that we consider\n",
    "for t in tweets_and_poster:\n",
    "    t['tweet'] = remove_stopwords(t['tweet'])\n",
    "\n",
    "# exemple of tweets without stopwords\n",
    "tweets_and_poster[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us build a function to get the embeddings of each tweet\n",
    "\n",
    "We will use the Word2Vec model that we previously loaded for question 2.ii.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings(tweet_and_poster):\n",
    "    tweet = tweet_and_poster['tweet']\n",
    "    if len(tweet)>0:\n",
    "        tokens = tweet.split()\n",
    "        embeddings = [w2v[token] for token in tokens if token in w2v]\n",
    "        \n",
    "        if len(tokens) > 0 and embeddings:\n",
    "                avg_embedding = np.mean(embeddings, axis=0)\n",
    "        else:\n",
    "            avg_embedding = np.zeros(w2v.vector_size)\n",
    "    else:\n",
    "         avg_embedding = np.zeros(w2v.vector_size)\n",
    "\n",
    "    return avg_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the weighted embeddings of our tweets and add them to our *tweets_and_poster* list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the weighted embedding key to the tweet and poster list of dictionnaries\n",
    "for t in tweets_and_poster:\n",
    "    t['weighted_embedding'] = embeddings(t)\n",
    "\n",
    "# exemple of embedding\n",
    "tweets_and_poster[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us build the top-k tweets retrieval of a query\n",
    "\n",
    "- Since the query is supposed to be built of keywords, it should not contain stopwords\n",
    "\n",
    "- If we compute the cosine similarity of the embedded query and all the embeddings, we can retrieve the top-k more relevant tweets\n",
    "\n",
    "- We can also account for posters centrality by multiplying the obtained cosine similarities by the degree centrality of the related poster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_retrieval(keywords, k):\n",
    "    print(keywords)\n",
    "    print() \n",
    "\n",
    "    embedding = [w2v[token] for token in keywords if token in w2v]\n",
    "    embedding = np.mean(embedding, axis=0).reshape(1, -1)\n",
    "\n",
    "    similarity_list = [cosine_similarity(embedding, i['weighted_embedding'].reshape(1, -1)) for i in tweets_and_poster]\n",
    "    weighted_similarity_list = [similarity_list[i] * tweets_and_poster[i]['deg_centrality']for i in range(len(similarity_list))]\n",
    "\n",
    "    related_tweets = [i['tweet'] for i in tweets_and_poster]\n",
    "\n",
    "    sorted_weighted_similarity_list = sorted(zip(weighted_similarity_list, related_tweets), reverse = True)\n",
    "    sorted_sims, sorted_tweets = zip(*sorted_weighted_similarity_list)\n",
    "\n",
    "    for i in range(len(sorted_sims[:k])):\n",
    "        print(f'Cosine Sim : {sorted_sims[i][0][0]} \\n Tweet : {sorted_tweets[i]} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['drive', 'fire', 'hospital', 'injury']\n",
    "k = 10\n",
    "\n",
    "top_k_retrieval(query, k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
